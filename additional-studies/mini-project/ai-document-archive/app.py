import streamlit as st
import torch
from sentence_transformers import SentenceTransformer
from transformers import (
    AutoModelForImageClassification, AutoProcessor, 
    AutoTokenizer, AutoModelForSeq2SeqLM,
    VisionEncoderDecoderModel, LayoutLMv3Processor, LayoutLMv3ForTokenClassification,
)

from paddleocr import PaddleOCR
from sqlmodel import Field, Session, SQLModel, create_engine, select
from datetime import datetime
from PIL import Image
import cv2
import io
import base64
import numpy as np
from typing import Optional
import json
import re
from sklearn.metrics.pairwise import cosine_similarity

from preprocess_image import *
from analyze_morpheme import *
from get_info_in_just_image import *
from extract_info import *

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
class Document(SQLModel, table=True):
    __table_args__ = {'extend_existing': True}
    id: Optional[int] = Field(default=None, primary_key=True)
    filename: str
    doc_type: str
    content: str
    summary: str
    keywords: str
    structured_data: str  # JSON í˜•íƒœë¡œ ì €ì¥
    upload_date: datetime = Field(default_factory=datetime.now)
    image_data: bytes
    embedding: Optional[str] = None  # ë²¡í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
engine = create_engine("sqlite:///archive.db")
SQLModel.metadata.create_all(engine)

# ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_models():
    # DiT ë¬¸ì„œ ë¶„ë¥˜
    dit_processor = AutoProcessor.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    dit_model = AutoModelForImageClassification.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    
    # OCR
    ocr = PaddleOCR(lang='korean')
    
    # Donut (ì˜ìˆ˜ì¦ ì „ìš©)
    donut_processor = AutoProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
    donut_model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
    
    # LayoutLMv3
    layout_processor = LayoutLMv3Processor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
    layout_model = LayoutLMv3ForTokenClassification.from_pretrained("microsoft/layoutlmv3-base")
    
    # í…ìŠ¤íŠ¸ ìš”ì•½
    summarizer_tokenizer = AutoTokenizer.from_pretrained("gangyeolkim/kobart-korean-summarizer-v2")
    summarizer_model = AutoModelForSeq2SeqLM.from_pretrained("gangyeolkim/kobart-korean-summarizer-v2")
    
    # ì„ë² ë”© ëª¨ë¸ (ë²¡í„° ê²€ìƒ‰ìš©)
    embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
    
    return (dit_processor, dit_model, ocr, donut_processor, donut_model, 
            layout_processor, layout_model, summarizer_tokenizer, summarizer_model,
            embedding_model)

# ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜
def classify_document(image, dit_processor, dit_model):
    inputs = dit_processor(images=image, return_tensors="pt")
    outputs = dit_model(**inputs)
    predicted_class_idx = outputs.logits.argmax(-1).item()
    predicted_class = dit_model.config.id2label[predicted_class_idx]
    
    # ì˜ìˆ˜ì¦ ê´€ë ¨ í´ë˜ìŠ¤ ë§¤í•‘
    if any(keyword in predicted_class.lower() for keyword in ['invoice', 'receipt']):
        return "ì˜ìˆ˜ì¦"
    
    # ë¬¸ì„œ ê´€ë ¨ í´ë˜ìŠ¤ê°€ ì•„ë‹ˆë©´ "ì¼ë°˜ ì‚¬ì§„"ìœ¼ë¡œ ë°˜í™˜
    if any(keyword in predicted_class.lower() for keyword in ['image', 'photo', 'landscape', 'nature']):
        return "ì¼ë°˜ ì‚¬ì§„"
    
    return predicted_class


# OCR ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ë¬¸ì„œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
def preprocess_image_for_ocr(image, blur_size, block_size, C_value, dilation_iter, to_grayscale):
    """OCR ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    # PILì„ openCVë¡œ ë³€í™˜
    img_np = np.array(image)  # Pillow â†’ numpy
    img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

    # 1. ê·¸ë ˆì´ ìŠ¤ì¼€ì¼ ë³€í™”
    if to_grayscale:
        gray_image = convert_to_grayscale(img_cv)
    else:
        gray_image = img_cv

    # 2. ë…¸ì´ì¦ˆ ì œê±°
    denoised = remove_noise(gray_image, blur_size)

    # 3. ëŒ€ë¹„ ê°œì„ 
    if to_grayscale:
        enhanced = improve_contrast(denoised)
    else:
        enhanced = denoised

    # 4. ì´ì§„í™” (í…ìŠ¤íŠ¸ì™€ ë°°ê²½ ë¶„ë¦¬)
    if to_grayscale:
        binary = apply_adaptive_binarization(enhanced, block_size, C_value)
    else:
        binary = enhanced

    # 5. í…ìŠ¤íŠ¸ ì˜ì—­ ê°•í™”
    final_image = enhance_text_regions(binary, dilation_iter)

    return final_image

# OCR ìˆ˜í–‰
def perform_ocr(image, ocr):
    result = ocr.ocr(np.array(image))
    return result

# í…ìŠ¤íŠ¸ì™€ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
def extract_text_and_positions(result):
    text = ""
    boxes = []
    
    if result[0]:
        for line in result[0]:
            text += line[1][0] + " "
            boxes.append(line[0])

    return text.strip(), boxes
    
# ê¸°ì¡´ OCR í•¨ìˆ˜ì— ì „ì²˜ë¦¬ ì˜µì…˜ ì¶”ê°€í•˜ê¸°
def extract_text_with_layout(ocr, image, blur_size, block_size, C_value, dilation_iter, to_grayscale, use_preprocessing=True):
    """ì „ì²˜ë¦¬ ì˜µì…˜ì´ ì¶”ê°€ëœ OCR í•¨ìˆ˜"""
    # ì „ì²˜ë¦¬ ì ìš© ì—¬ë¶€ í™•ì¸í•˜ê¸°
    if use_preprocessing:
        image = preprocess_image_for_ocr(image, blur_size, block_size, C_value, dilation_iter, to_grayscale)

    # OCR ìˆ˜í–‰í•˜ê¸°
    result = perform_ocr(image, ocr)

    # í…ìŠ¤íŠ¸ì™€ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œí•˜ê¸°
    text, boxes = extract_text_and_positions(result)
    
    return text, boxes

#---------------------------------------------------------------------------------
# í…ìŠ¤íŠ¸ ìš”ì•½
def summarize_text(text, tokenizer, model):
    if not text or len(text) < 50:
        return text
    
    inputs = tokenizer(text[:1024], return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(inputs["input_ids"], max_length=128, min_length=20, num_beams=4)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
def create_embedding(text, model):
    if not text or len(text.strip()) < 2:
        return None
    
    # ì§ì ‘ ë¬¸ì¥ ì„ë² ë”© ìƒì„± (í›¨ì”¬ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì )
    embedding = model.encode(text)
    return embedding.tolist()

# êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ê¸° í•¨ìˆ˜
def extract_structured_keywords(structured_data):
    structured_data_keywords = []

    if 'store' in structured_data:
        structured_data_keywords.append(structured_data['store'])
    if 'date' in structured_data:
        structured_data_keywords.append(structured_data['date'])

    return structured_data_keywords

# ê¸°ì¡´ í•¨ìˆ˜ ëŒ€ì²´í•˜ê¸°
def extract_keywords(text, structured_data=None):
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜"""
    # í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords, method = extract_keywords_with_morpheme_analysis(text)
    
    # êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ
    if structured_data:
        keywords.extend(extract_structured_keywords(structured_data))
    
    return ", ".join(list(set(keywords)))

#---------------------------------------------------------------------------
# ì‚¬ì§„ êµ¬ë¶„ ë° ë©”íƒ€ë°ì´í„° ê²€ìƒ‰
# ì‚¬ì§„ ì•ˆì˜ ê°ì²´ íƒì§€í•˜ê¸°
def detect_photo_objects(image):
    """ì‚¬ì§„ ë‚´ì˜ ê°ì²´ íƒì§€"""
    detected_objects = run_object_detection_model(image)

    # ë””ë²„ê¹…
    for obj in detected_objects:
        print(f"Detected {obj['class']} with confidence {obj['confidence']:.2f}")

    return detected_objects

#------------------------------------------------------------
# ë¬¸ì„œ ì²˜ë¦¬
def process_document(uploaded_file, models, blur_size, block_size, C_value, dilation_iter, to_grayscale):
    (dit_processor, dit_model, ocr, donut_processor, donut_model, 
     layout_processor, layout_model, sum_tokenizer, sum_model,
     embedding_model) = models
    
    # ì´ë¯¸ì§€ë¥¼ Pillowë¡œ ë³€í™˜ -> RGB
    image = Image.open(uploaded_file).convert("RGB")
    
    # 1. ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜
    doc_type = classify_document(image, dit_processor, dit_model)
    
    print('\n==========')
    print(f'\n[doc_type]\n{doc_type}')
    
    content, boxes = extract_text_with_layout(image, ocr, blur_size, block_size, C_value, dilation_iter, to_grayscale)
    layoutlm_data = extract_structured_with_layoutlm(image, content, boxes, layout_processor, layout_model, doc_type)
    
    print('\n==========')
    print(f'\n[layoutlm_data]\n{layoutlm_data}')
    
    # ì´ê±° ì–´ë””ì— ë“¤ì–´ê°€ì•¼ í•˜ëŠ”ì§€ ìƒê°í•˜ê¸°
    # ì‚¬ì§„ ì—¬ë¶€ íŒë³„í•˜ê¸°
    if is_photo(doc_type, content):
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = extract_photo_metadata(image)
        
        # ê°ì²´ íƒì§€
        objects = detect_photo_objects(image)

        # í‚¤ì›Œë“œ ìƒì„±
        keywords = generate_photo_keywords(metadata, objects)

        # êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ì €ì¥
        structured_data = format_photo_data(metadata, objects)

        # ìš”ì•½ ìƒì„±
        summary = create_photo_summary(objects, metadata)

        # ì„ë² ë”© ìƒì„±
        embedding = create_embedding(summary, embedding_model)

        return doc_type, content, summary, keywords, structured_data, img_data, embedding

    # 2. ë¬¸ì„œ ìœ í˜•ë³„ ì²˜ë¦¬
    if doc_type == "ì˜ìˆ˜ì¦":
        print('ì˜ìˆ˜ì¦')
        # Donutìœ¼ë¡œ ì˜ìˆ˜ì¦ ì •ë³´ ì¶”ì¶œ
        receipt_info = extract_receipt_info(image, donut_processor, donut_model)
        print('\n==========')
        print(f'\n[receipt_info]\n{receipt_info}')
        
        # êµ¬ì¡°í™”ëœ ì •ë³´ ë³‘í•©
        structured_data = extract_structured_info(content, doc_type)
        if receipt_info:
            # Donut ê²°ê³¼ê°€ ìˆìœ¼ë©´ LayoutMLê³¼ ë³‘í•©    
            for key, value in receipt_info.items():
                if key not in layoutlm_data or not layoutlm_data[key]:
                    layoutlm_data[key] = value
        else:
            structured_data = layoutlm_data
        structured_data.update(receipt_info)
    else:
        # ì¼ë°˜ OCR
        structured_data = layoutlm_data

    print('\n==========')
    print(f'\n[content]\n{content}')
    print('\n==========')
    print(f'\n[structured_data]\n{structured_data}')
        
    # 3. ìš”ì•½ ìƒì„±
    summary = summarize_text(content, sum_tokenizer, sum_model)

    print('\n==========')
    print(f'\n[summary]\n{summary}')
    
    # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(content, structured_data)
    print('\n==========')
    print(f'\n[keywords]\n{keywords}')
    
    # 5. ì„ë² ë”© ìƒì„±
    embedding = create_embedding(content + " " + summary, embedding_model)
    
    # ì´ë¯¸ì§€ ì €ì¥
    img_byte_arr = io.BytesIO()
    image.save(img_byte_arr, format='PNG')
    img_data = img_byte_arr.getvalue()

    return doc_type, content, summary, keywords, structured_data, img_data, embedding

# ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
def search_by_similarity(query, embedding_model, session):
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = create_embedding(query, embedding_model)
    
    # ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
    all_docs = session.exec(select(Document)).all()
    
    similarities = []
    for doc in all_docs:
        if doc.embedding:
            doc_embedding = json.loads(doc.embedding)
            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]
            similarities.append((doc, similarity))
    
    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    similarities.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, sim in similarities[:10] if sim > 0.5]

# ê²°ê³¼ ëª©ë¡ ì¶œë ¥
def print_result_list(results):
    for doc in results:
        with st.expander(f"{doc.filename} - {doc.upload_date.strftime('%Y-%m-%d %H:%M')}"):
            col1, col2 = st.columns(2)
            with col1:
                img = Image.open(io.BytesIO(doc.image_data))
                st.image(img, use_container_width=True)
            with col2:
                st.write(f"**ë¬¸ì„œ ìœ í˜•:** {doc.doc_type}")
                st.write(f"**ìš”ì•½:** {doc.summary}")
                st.write(f"**í‚¤ì›Œë“œ:** {doc.keywords}")
                
                if doc.structured_data:
                    structured = json.loads(doc.structured_data)
                    if structured:
                        st.write("**ì¶”ì¶œëœ ì •ë³´:**")
                        for k, v in structured.items():
                            st.write(f"- {k}: {v}")
                
                # ë‹¤ìš´ë¡œë“œ
                b64 = base64.b64encode(doc.image_data).decode()
                href = f'<a href="data:image/png;base64,{b64}" download="{doc.filename}">ë‹¤ìš´ë¡œë“œ</a>'
                st.markdown(href, unsafe_allow_html=True)

#-------------------------------------------------------------------------------------------------
# Streamlit UI
st.title("AI ì•„ì¹´ì´ë¸Œ ì‹œìŠ¤í…œ")

# ëª¨ë¸ ë¡œë“œ
with st.spinner("AI ëª¨ë¸ ë¡œë”© ì¤‘..."):
    models = load_models()

# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["ë¬¸ì„œ ì—…ë¡œë“œ", "ë¬¸ì„œ ê²€ìƒ‰", "ë¬¸ì„œ ëª©ë¡"])

if 'processed_file' not in st.session_state:
    st.session_state.processed_file = None
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'doc_results' not in st.session_state:
    st.session_state.doc_results = None

# ë¬¸ì„œ ì—…ë¡œë“œ íƒ­
with tab1:
    uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['png', 'jpg', 'jpeg'])

    # ìƒˆ íŒŒì¼ì´ë©´ ì„¸ì…˜ ì´ˆê¸°í™” í•˜ê¸° + ì´ì „ ê²°ê³¼ ë°±ì—…í•˜ê¸°
    if uploaded_file is not None and uploaded_file != st.session_state.processed_file:
        # ì´ì „ ê²°ê³¼ ë°±ì—…
        if st.session_state.doc_results:
            st.session_state.prev_doc_results = st.session_state.doc_results.copy()

        st.session_state.processed_file = uploaded_file
        st.session_state.processing_complete = False
        st.session_state.ocr_ready = False    # OCRì€ ë²„íŠ¼ ëˆ„ë¥¼ ë•Œë§Œ ì§„í–‰í•œë‹¤.

    # íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆë‹¤ë©´
    if uploaded_file:
        st.subheader("ğŸ”§ ì „ì²˜ë¦¬ ì„¤ì •")

        grayscale_option = st.checkbox("í‘ë°±(Grayscale) ì²˜ë¦¬", value=True)
        blur_size = st.slider("ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì»¤ë„ í¬ê¸°", 1, 11, 5, step=2)
        block_size = st.slider("Adaptive Threshold blockSize", 3, 25, 11, step=2)
        C_value = st.slider("Threshold ìƒìˆ˜ C", 0, 10, 2)
        dilation_iter = st.slider("Dilation ë°˜ë³µ íšŸìˆ˜", 0, 3, 1)

        # ì „ì²˜ë¦¬ ë¯¸ë¦¬ë³´ê¸°
        pil_image = Image.open(uploaded_file).convert("RGB")
        final_image = preprocess_image_for_ocr(pil_image, blur_size, block_size, C_value, dilation_iter, grayscale_option)

        if grayscale_option:
            channel_name = "GRAY"
        else:
            channel_name = "COLOR"
        
        st.image(final_image, caption="ì „ì²˜ë¦¬ ê²°ê³¼", channels=channel_name)
        
        if st.button("ğŸ”  OCR ì‹œì‘"):
            with st.spinner("OCR ë¶„ì„ ì¤‘..."):
                doc_type, content, summary, keywords, structured_data, img_data, embedding = process_document(
                    uploaded_file, models, blur_size, block_size, C_value, dilation_iter, grayscale_option
                )
                st.session_state.processing_complete = True
                st.session_state.ocr_ready = True
                # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
                st.session_state.doc_results = {
                    'doc_type': doc_type,
                    'content': content,
                    'summary': summary,
                    'keywords': keywords,
                    'structured_data': structured_data,
                    'img_data': img_data,
                    'embedding': embedding
                }
        
    prev_results = st.session_state.get("prev_doc_results")

    # ì²˜ë¦¬ ì™„ë£Œëœ ê²°ê³¼ í‘œì‹œ
    if uploaded_file is not None and st.session_state.doc_results is not None:
        results = st.session_state.doc_results
        

        st.subheader("ğŸ“„ í˜„ì¬ ë¶„ì„ëœ ë¬¸ì„œ ê²°ê³¼")
        
        col1, col2 = st.columns(2)
        with col1:
            st.image(uploaded_file, caption="ì—…ë¡œë“œëœ ë¬¸ì„œ", use_container_width=True)
        
        with col2:
            st.write(f"**ë¬¸ì„œ ìœ í˜•:** {results['doc_type']}")
            st.write(f"**ìš”ì•½:** {results['summary']}")
            st.write(f"**í‚¤ì›Œë“œ:** {results['keywords']}")
            
            if results['structured_data']:
                st.write("**ì¶”ì¶œëœ ì •ë³´:**")
                for key, value in results['structured_data'].items():
                    st.write(f"- {key}: {value}")
            
            if st.button("ì €ì¥"):
                with Session(engine) as session:
                    doc = Document(
                        filename=uploaded_file.name,
                        doc_type=results['doc_type'],
                        content=results['content'],
                        summary=results['summary'],
                        keywords=results['keywords'],
                        structured_data=json.dumps(results['structured_data'], ensure_ascii=False),
                        image_data=results['img_data'],
                        embedding=json.dumps(results['embedding'])
                    )
                    session.add(doc)
                    session.commit()
                st.success("ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.session_state.processed_file = None
                st.session_state.processing_complete = False
                st.session_state.doc_results = None

    # ì´ì „ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë¹„êµí•˜ê¸°
    if prev_results:
        st.markdown("---")
        st.subheader("ğŸ” ì´ì „ ë¶„ì„ ê²°ê³¼ì™€ ë¹„êµ")

        col1, col2 = st.columns(2)

        with col1:
            st.write("ğŸ“„ **ì´ì „ ìš”ì•½**")
            st.write(prev_results["summary"])
            st.write("ğŸ”‘ **ì´ì „ í‚¤ì›Œë“œ**")
            st.write(prev_results["keywords"])

            if prev_results.get("structured_data"):
                st.write("ğŸ“‹ **ì´ì „ ì¶”ì¶œ ì •ë³´**")
                for key, value in prev_results["structured_data"].items():
                    st.write(f"- {key}: {value}")

        with col2:
            st.write("ğŸ“„ **í˜„ì¬ ìš”ì•½**")
            st.write(results["summary"])
            st.write("ğŸ”‘ **í˜„ì¬ í‚¤ì›Œë“œ**")
            st.write(results["keywords"])

            if results.get("structured_data"):
                st.write("ğŸ“‹ **í˜„ì¬ ì¶”ì¶œ ì •ë³´**")
                for key, value in results["structured_data"].items():
                    st.write(f"- {key}: {value}")

# ë¬¸ì„œ ê²€ìƒ‰ íƒ­
with tab2:
    search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì»¤í”¼ ì˜ìˆ˜ì¦)")
    search_method = st.radio("ê²€ìƒ‰ ë°©ë²•", ["ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰", "í‚¤ì›Œë“œ ê²€ìƒ‰"])
    
    if st.button("ê²€ìƒ‰", key='search_button'):
        with Session(engine) as session:
            if search_method == "ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰":
                results = search_by_similarity(
                    search_query, 
                    models[9],  # embedding_model
                    session
                )
            else:
                # í‚¤ì›Œë“œ ê²€ìƒ‰
                statement = select(Document).where(
                    Document.keywords.contains(search_query) | 
                    Document.summary.contains(search_query) |
                    Document.doc_type.contains(search_query)
                )
                results = session.exec(statement).all()
            
            if results:
                print_result_list(results)
            else:
                st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ë¬¸ì„œ ëª©ë¡ íƒ­
with tab3:
    with Session(engine) as session:
        statement = select(Document)
        results = session.exec(statement).all()
        if results:
            print_result_list(results)
    