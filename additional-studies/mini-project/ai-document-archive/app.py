import streamlit as st
import torch
from sentence_transformers import SentenceTransformer
from transformers import (
    AutoModelForImageClassification, AutoProcessor, 
    AutoTokenizer, AutoModelForSeq2SeqLM,
    VisionEncoderDecoderModel, LayoutLMv3Processor, LayoutLMv3ForTokenClassification,
)
from paddleocr import PaddleOCR
from sqlmodel import Field, Session, SQLModel, create_engine, select
from datetime import datetime
from PIL import Image
import cv2
import io
import base64
import numpy as np
from typing import Optional
import json
import re
from sklearn.metrics.pairwise import cosine_similarity

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
class Document(SQLModel, table=True):
    __table_args__ = {'extend_existing': True}
    id: Optional[int] = Field(default=None, primary_key=True)
    filename: str
    doc_type: str
    content: str
    summary: str
    keywords: str
    structured_data: str  # JSON í˜•íƒœë¡œ ì €ì¥
    upload_date: datetime = Field(default_factory=datetime.now)
    image_data: bytes
    embedding: Optional[str] = None  # ë²¡í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
engine = create_engine("sqlite:///archive.db")
SQLModel.metadata.create_all(engine)

# ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_models():
    # DiT ë¬¸ì„œ ë¶„ë¥˜
    dit_processor = AutoProcessor.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    dit_model = AutoModelForImageClassification.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    
    # OCR
    ocr = PaddleOCR(lang='korean')
    
    # Donut (ì˜ìˆ˜ì¦ ì „ìš©)
    donut_processor = AutoProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
    donut_model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
    
    # LayoutLMv3
    layout_processor = LayoutLMv3Processor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
    layout_model = LayoutLMv3ForTokenClassification.from_pretrained("microsoft/layoutlmv3-base")
    
    # í…ìŠ¤íŠ¸ ìš”ì•½
    summarizer_tokenizer = AutoTokenizer.from_pretrained("gangyeolkim/kobart-korean-summarizer-v2")
    summarizer_model = AutoModelForSeq2SeqLM.from_pretrained("gangyeolkim/kobart-korean-summarizer-v2")
    
    # ì„ë² ë”© ëª¨ë¸ (ë²¡í„° ê²€ìƒ‰ìš©)
    embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
    
    return (dit_processor, dit_model, ocr, donut_processor, donut_model, 
            layout_processor, layout_model, summarizer_tokenizer, summarizer_model,
            embedding_model)

# ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜
def classify_document(image, dit_processor, dit_model):
    inputs = dit_processor(images=image, return_tensors="pt")
    outputs = dit_model(**inputs)
    predicted_class_idx = outputs.logits.argmax(-1).item()
    predicted_class = dit_model.config.id2label[predicted_class_idx]
    
    # ì˜ìˆ˜ì¦ ê´€ë ¨ í´ë˜ìŠ¤ ë§¤í•‘
    if any(keyword in predicted_class.lower() for keyword in ['invoice']):
        return "ì˜ìˆ˜ì¦"
    return predicted_class

# ì˜ìˆ˜ì¦ OCR (Donut)
def extract_receipt_info(image, processor, model):
    pixel_values = processor(image, return_tensors="pt").pixel_values
    decoder_input_ids = processor.tokenizer("<s_cord-v2>", return_tensors="pt").input_ids
    
    outputs = model.generate(pixel_values, decoder_input_ids=decoder_input_ids, max_length=512)
    prediction = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    
    # JSON íŒŒì‹±
    try:
        receipt_data = json.loads(prediction)
        return receipt_data
    except:
        return {}


# ê·¸ë ˆì´ ìŠ¤ì¼€ì¼ ë³€í™˜í•˜ê¸°
def convert_to_grayscale(image):
    img_np = np.array(image)  # Pillow â†’ numpy
    gray_image = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)
    return gray_image

# ë…¸ì´ì¦ˆ ì œê±°í•˜ê¸°
def remove_noise(gray_image, blur_size):
    # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì‚¬ìš© => ì»¤ë„ì˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ë” ë¶€ë“œëŸ½ê²Œ ëœë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ í¬ë©´ ë­‰ê°œì§.
    # ì–‡ì€ ê¸€ìì¼ ê²½ìš° -> ì‘ê²Œ / ì‰í¬ ë²ˆì§ê³¼ ê°™ì€ ë…¸ì´ì¦ˆê°€ ë§ì„ ê²½ìš° -> ì¡°ê¸ˆ ë” í¬ê²Œ
    denoised = cv2.GaussianBlur(gray_image, (blur_size, blur_size), 0)
    return denoised

# ëŒ€ë¹„ ê°œì„ í•˜ê¸°
def improve_contrast(denoised):
    # ì „ì²´ì ì¸ ëŒ€ë¹„ í–¥ìƒ -> ì¡°ëª…ì— ë”°ë¼ ë¶€ìì—°ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆìŒ.
    # enhanced = cv2.equalizeHist(denoised)
    # ë” ì¢‹ì€ ëŒ€ì•ˆ : CLAHE (ì ì‘ì  íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”)
    # clipLimitê°€ ë‚®ì„ìˆ˜ë¡ ë¶€ë“œëŸ½ê³ , ë†’ì„ìˆ˜ë¡ ëŒ€ë¹„ê°€ ê°•ì¡°ëœë‹¤.
    # tileGridSizeëŠ” ê¸€ì í¬ê¸° ê¸°ì¤€ ë³´í†µ (8, 8), ê¸€ìê°€ ì‘ìœ¼ë©´ (4, 4)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(denoised) 
    return enhanced

# ì´ì§„í™”
def apply_adaptive_binarization(enhanced, block_size, C_value):
    binary = cv2.adaptiveThreshold(
        enhanced, 255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2. THRESH_BINARY,
        block_size,     # ì£¼ë³€ ë¸”ë¡ í¬ê¸°. ê¸€ì í¬ê¸°ë³´ë‹¤ ì¡°ê¸ˆ í° ê°’ì´ ì¢‹ìŒ.
        C_value       # ë¹¼ëŠ” ìƒìˆ˜. ì–´ë‘ìš´ ë°°ê²½ì¼ìˆ˜ë¡ ì¡°ê¸ˆ ë” í¬ê²Œ ì¡°ì ˆ ê°€ëŠ¥.
    )
    return binary

# í…ìŠ¤íŠ¸ ì˜ì—­ ê°•í™”
def enhance_text_regions(binary, dilation_iter):
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    # í…ìŠ¤íŠ¸ êµµê²Œ í•´ì„œ OCRì´ ì˜ ë˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤.
    # ì–‡ì€ ê¸€ì”¨ : iterations=1
    # ëŠê¸´ ê¸€ì”¨ë‚˜ ë²ˆì§„ ì‰í¬ëŠ” (3, 3)ë³´ë‹¤ í° ì»¤ë„ë¡œ iterations=2~3ë„ ì‹¤í—˜í•´ë³¼ë§Œí•˜ë‹¤.
    final_image = cv2.dilate(binary, kernel, iterations=dilation_iter)
    return final_image

# OCR ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
def preprocess_image_for_ocr(image, blur_size, block_size, C_value, dilation_iter):
    """OCR ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    # 1. ê·¸ë ˆì´ ìŠ¤ì¼€ì¼ ë³€í™”
    gray_image = convert_to_grayscale(image)

    # 2. ë…¸ì´ì¦ˆ ì œê±°
    denoised = remove_noise(gray_image, blur_size)

    # 3. ëŒ€ë¹„ ê°œì„ 
    enhanced = improve_contrast(denoised)

    # 4. ì´ì§„í™” (í…ìŠ¤íŠ¸ì™€ ë°°ê²½ ë¶„ë¦¬)
    binary = apply_adaptive_binarization(enhanced, block_size, C_value)

    # 5. í…ìŠ¤íŠ¸ ì˜ì—­ ê°•í™”
    final_image = enhance_text_regions(binary, dilation_iter)

    return final_image


# OCR ìˆ˜í–‰
def perform_ocr(image):
    result = PaddleOCR(lang='korean').ocr(np.array(image))
    return result


# í…ìŠ¤íŠ¸ì™€ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
def extract_text_and_positions(result):
    text = ""
    boxes = []
    
    if result[0]:
        for line in result[0]:
            text += line[1][0] + " "
            boxes.append(line[0])

    return text.strip(), boxes
    

# ê¸°ì¡´ OCR í•¨ìˆ˜ì— ì „ì²˜ë¦¬ ì˜µì…˜ ì¶”ê°€í•˜ê¸°
def extract_text_with_layout(image, blur_size, block_size, C_value, dilation_iter, use_preprocessing=True):
    """ì „ì²˜ë¦¬ ì˜µì…˜ì´ ì¶”ê°€ëœ OCR í•¨ìˆ˜"""
    # ì „ì²˜ë¦¬ ì ìš© ì—¬ë¶€ í™•ì¸í•˜ê¸°
    if use_preprocessing:
        image = preprocess_image_for_ocr(image, blur_size, block_size, C_value, dilation_iter)

    # OCR ìˆ˜í–‰í•˜ê¸°
    result = perform_ocr(image)

    # í…ìŠ¤íŠ¸ì™€ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œí•˜ê¸°
    text, boxes = extract_text_and_positions(result)
    
    return text, boxes


# LayoutLMv3ë¥¼ í™œìš©í•œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜
def extract_structured_with_layoutlm(image, text, boxes, layout_processor, layout_model, doc_type):
    """
    LayoutLMv3ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì£¼ì˜: ê¸°ë³¸ LayoutLMv3ëŠ” íŠ¹ì • íƒœìŠ¤í¬ì— fine-tuningë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ,
    ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì •ë³´ë¥¼ í™œìš©í•œ íœ´ë¦¬ìŠ¤í‹± ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        image: PIL Image ê°ì²´
        text: OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸
        boxes: OCRë¡œ ì¶”ì¶œí•œ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
        layout_processor: LayoutLMv3 í”„ë¡œì„¸ì„œ
        layout_model: LayoutLMv3 ëª¨ë¸
        doc_type: ë¬¸ì„œ ìœ í˜• (ì˜ìˆ˜ì¦, ë¬¸ì„œ ë“±)
    
    Returns:
        structured_data: ì¶”ì¶œëœ êµ¬ì¡°í™”ëœ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    # OCR ê²°ê³¼ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    words = text.split()
    
    # ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ ë‹¨ì–´ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    if not words:
        return {}
    
    # ë°”ìš´ë”© ë°•ìŠ¤ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•œ ê²½ìš° ì²˜ë¦¬
    if not boxes or len(boxes) == 0:
        # ë°•ìŠ¤ ì •ë³´ ì—†ì´ ê¸°ë³¸ êµ¬ì¡°ë§Œ ë°˜í™˜
        if doc_type == "ì˜ìˆ˜ì¦":
            return {"í…ìŠ¤íŠ¸": text}
        else:
            return {"ë‚´ìš©": text}
    
    # ë°”ìš´ë”© ë°•ìŠ¤ê°€ ë‹¨ì–´ ìˆ˜ë³´ë‹¤ ì ì„ ê²½ìš° ì²˜ë¦¬
    if len(boxes) < len(words):
        # ë§ˆì§€ë§‰ ë°•ìŠ¤ë¥¼ ë³µì‚¬í•˜ì—¬ ë§ì¶¤
        boxes = boxes + [boxes[-1]] * (len(words) - len(boxes))
    
    # ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì •ê·œí™” (0-1000 ë²”ìœ„ë¡œ)
    width, height = image.size
    normalized_boxes = []
    word_positions = []  # ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ ì €ì¥
    
    for idx, box in enumerate(boxes[:len(words)]):
        # boxëŠ” [[x1,y1], [x2,y1], [x2,y2], [x1,y2]] í˜•íƒœ
        if len(box) >= 4:
            x1, y1 = box[0]
            x2, y2 = box[2]
            # LayoutLMì€ [x1, y1, x2, y2] í˜•íƒœë¡œ 0-1000 ë²”ìœ„ ì¢Œí‘œ í•„ìš”
            norm_box = [
                int(x1 * 1000 / width),
                int(y1 * 1000 / height),
                int(x2 * 1000 / width),
                int(y2 * 1000 / height)
            ]
            normalized_boxes.append(norm_box)
            
            # ìœ„ì¹˜ ì •ë³´ ì €ì¥ (ìƒëŒ€ì  ìœ„ì¹˜)
            word_positions.append({
                'word': words[idx] if idx < len(words) else '',
                'x_center': (norm_box[0] + norm_box[2]) / 2,
                'y_center': (norm_box[1] + norm_box[3]) / 2,
                'width': norm_box[2] - norm_box[0],
                'height': norm_box[3] - norm_box[1],
                'area': (norm_box[2] - norm_box[0]) * (norm_box[3] - norm_box[1])
            })
    
    # LayoutLMv3 ì…ë ¥ ì¤€ë¹„
    encoding = layout_processor(
        image,
        words[:len(normalized_boxes)],
        boxes=normalized_boxes,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=512
    )
    
    # ëª¨ë¸ ì¶”ë¡  - ì„ë² ë”© ì¶”ì¶œ
    with torch.no_grad():
        outputs = layout_model(**encoding, output_hidden_states=True)
    
    # ë§ˆì§€ë§‰ ì€ë‹‰ì¸µì˜ ì„ë² ë”© ì‚¬ìš©
    last_hidden_states = outputs.hidden_states[-1]
    
    # ë¬¸ì„œ ìœ í˜•ë³„ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
    if doc_type == "ì˜ìˆ˜ì¦":
        structured_data = extract_receipt_structure(word_positions, words, last_hidden_states)
    else:
        structured_data = extract_document_structure(word_positions, words, last_hidden_states)
    
    return structured_data


def extract_receipt_structure(word_positions, words, embeddings):
    """
    ì˜ìˆ˜ì¦ ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ìœ„ì¹˜ ì •ë³´ì™€ í…ìŠ¤íŠ¸ íŒ¨í„´ì„ í™œìš©í•œ íœ´ë¦¬ìŠ¤í‹± ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    structured_data = {
        "ìƒí˜¸ëª…": None,
        "ë‚ ì§œ": None,
        "ì‹œê°„": None,
        "í’ˆëª©": [],
        "ê¸ˆì•¡": [],
        "í•©ê³„": None,
        "ì£¼ì†Œ": None,
        "ì „í™”ë²ˆí˜¸": None,
        "ì‚¬ì—…ìë²ˆí˜¸": None
    }
    
    # ìœ„ì¹˜ë³„ë¡œ ë‹¨ì–´ ê·¸ë£¹í™”
    # ìƒë‹¨ 20% - ì£¼ë¡œ ìƒí˜¸ëª…, ì£¼ì†Œ
    # ì¤‘ê°„ 60% - í’ˆëª©ê³¼ ê¸ˆì•¡
    # í•˜ë‹¨ 20% - í•©ê³„, ì‚¬ì—…ìë²ˆí˜¸ ë“±
    
    top_words = []
    middle_words = []
    bottom_words = []
    
    for pos in word_positions:
        if pos['y_center'] < 200:  # ìƒë‹¨
            top_words.append(pos)
        elif pos['y_center'] < 800:  # ì¤‘ê°„
            middle_words.append(pos)
        else:  # í•˜ë‹¨
            bottom_words.append(pos)
    
    # 1. ìƒí˜¸ëª… ì¶”ì¶œ (ìƒë‹¨ì—ì„œ ê°€ì¥ í° í…ìŠ¤íŠ¸)
    if top_words:
        # ë©´ì ì´ ê°€ì¥ í° í…ìŠ¤íŠ¸ë¥¼ ìƒí˜¸ëª…ìœ¼ë¡œ ì¶”ì •
        largest = max(top_words, key=lambda x: x['area'])
        structured_data["ìƒí˜¸ëª…"] = largest['word']
    
    # 2. ë‚ ì§œì™€ ì‹œê°„ ì¶”ì¶œ
    for pos in word_positions:
        word = pos['word']
        # ë‚ ì§œ íŒ¨í„´
        date_match = re.search(r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})', word)
        if date_match:
            structured_data["ë‚ ì§œ"] = word
        
        # ì‹œê°„ íŒ¨í„´
        time_match = re.search(r'(\d{1,2}):(\d{2})', word)
        if time_match:
            structured_data["ì‹œê°„"] = word
    
    # 3. ê¸ˆì•¡ ì¶”ì¶œ (ì¤‘ê°„ ì˜ì—­ì—ì„œ ìš°ì¸¡ì— ìœ„ì¹˜í•œ ìˆ«ì)
    amount_pattern = r'[\d,]+ì›?'
    for pos in middle_words:
        word = pos['word']
        if re.match(amount_pattern, word) and pos['x_center'] > 500:  # ìš°ì¸¡
            # ìˆ«ìë§Œ ì¶”ì¶œ (ì½¤ë§ˆ, ì›, ê¸°íƒ€ ë¬¸ì ì œê±°)
            cleaned_amount = re.sub(r'[^\d]', '', word)
            if cleaned_amount and cleaned_amount.isdigit():
                structured_data["ê¸ˆì•¡"].append(cleaned_amount)
    
    # 4. í’ˆëª© ì¶”ì¶œ (ì¤‘ê°„ ì˜ì—­ì—ì„œ ì¢Œì¸¡ì— ìœ„ì¹˜í•œ í…ìŠ¤íŠ¸)
    for pos in middle_words:
        word = pos['word']
        if pos['x_center'] < 500 and not re.match(r'[\d,]+', word):  # ì¢Œì¸¡, ìˆ«ì ì•„ë‹˜
            structured_data["í’ˆëª©"].append(word)
    
    # 5. í•©ê³„ ì¶”ì¶œ (í•˜ë‹¨ì—ì„œ ê°€ì¥ í° ê¸ˆì•¡)
    total_candidates = []
    for pos in bottom_words:
        word = pos['word']
        if 'í•©ê³„' in word or 'ì´' in word:
            # í•©ê³„ í‚¤ì›Œë“œ ê·¼ì²˜ì˜ ê¸ˆì•¡ ì°¾ê¸°
            idx = word_positions.index(pos)
            for i in range(max(0, idx-3), min(len(word_positions), idx+4)):
                nearby_word = word_positions[i]['word']
                if re.match(r'[\d,]+ì›?', nearby_word):
                    # ìˆ«ìë§Œ ì¶”ì¶œ
                    cleaned_amount = re.sub(r'[^\d]', '', nearby_word)
                    if cleaned_amount and cleaned_amount.isdigit():
                        amount = int(cleaned_amount)
                        total_candidates.append(amount)
    
    if total_candidates:
        structured_data["í•©ê³„"] = str(max(total_candidates))
    elif structured_data["ê¸ˆì•¡"]:
        # í•©ê³„ê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš° ê¸ˆì•¡ ì¤‘ ìµœëŒ€ê°’
        try:
            amounts = []
            for x in structured_data["ê¸ˆì•¡"]:
                if x.isdigit():
                    amounts.append(int(x))
            if amounts:
                structured_data["í•©ê³„"] = str(max(amounts))
        except (ValueError, AttributeError):
            pass  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ í•©ê³„ ìƒëµ
    
    # 6. ì‚¬ì—…ìë²ˆí˜¸ ì¶”ì¶œ
    for word in words:
        # ì‚¬ì—…ìë²ˆí˜¸ íŒ¨í„´ (XXX-XX-XXXXX)
        business_match = re.search(r'\d{3}-\d{2}-\d{5}', word)
        if business_match:
            structured_data["ì‚¬ì—…ìë²ˆí˜¸"] = business_match.group()
    
    # 7. ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
    for word in words:
        # ì „í™”ë²ˆí˜¸ íŒ¨í„´
        phone_match = re.search(r'(\d{2,3})-(\d{3,4})-(\d{4})', word)
        if phone_match:
            structured_data["ì „í™”ë²ˆí˜¸"] = phone_match.group()
    
    # None ê°’ ì œê±°
    structured_data = {k: v for k, v in structured_data.items() if v}
    
    return structured_data


def extract_document_structure(word_positions, words, embeddings):
    """
    ì¼ë°˜ ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    structured_data = {
        "ì œëª©": None,
        "ë¶€ì œëª©": [],
        "ë‚ ì§œ": None,
        "ì‘ì„±ì": None,
        "í•µì‹¬ë‚´ìš©": [],
        "ë²ˆí˜¸ì •ë³´": []  # ì „í™”ë²ˆí˜¸, ê³„ì¢Œë²ˆí˜¸ ë“±
    }
    
    # 1. ì œëª© ì¶”ì¶œ (ìƒë‹¨ì—ì„œ ê°€ì¥ í° í…ìŠ¤íŠ¸)
    top_words = [pos for pos in word_positions if pos['y_center'] < 200]
    if top_words:
        largest = max(top_words, key=lambda x: x['area'])
        structured_data["ì œëª©"] = largest['word']
    
    # 2. ë¶€ì œëª© ì¶”ì¶œ (ì¤‘ê°„ í¬ê¸°ì˜ í…ìŠ¤íŠ¸)
    avg_area = sum(pos['area'] for pos in word_positions) / len(word_positions)
    for pos in word_positions:
        if pos['area'] > avg_area * 1.5 and pos['word'] != structured_data.get("ì œëª©"):
            structured_data["ë¶€ì œëª©"].append(pos['word'])
    
    # 3. ë‚ ì§œ ì¶”ì¶œ
    for word in words:
        date_match = re.search(r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})', word)
        if date_match:
            structured_data["ë‚ ì§œ"] = date_match.group()
            break
    
    # 4. ë²ˆí˜¸ ì •ë³´ ì¶”ì¶œ
    for word in words:
        # ì „í™”ë²ˆí˜¸
        phone_match = re.search(r'(\d{2,3})-(\d{3,4})-(\d{4})', word)
        if phone_match:
            structured_data["ë²ˆí˜¸ì •ë³´"].append(f"ì „í™”: {phone_match.group()}")
        
        # ê³„ì¢Œë²ˆí˜¸ íŒ¨í„´ (ìˆ«ìê°€ 10ìë¦¬ ì´ìƒ)
        account_match = re.search(r'\d{10,}', word)
        if account_match:
            structured_data["ë²ˆí˜¸ì •ë³´"].append(f"ê³„ì¢Œ: {account_match.group()}")
    
    # 5. í•µì‹¬ ë‚´ìš© ì¶”ì¶œ (í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ê¸´ ë¶€ë¶„)
    long_texts = []
    current_text = []
    
    for i, word in enumerate(words):
        current_text.append(word)
        # ë¬¸ì¥ ì¢…ë£Œ íŒë‹¨
        if word.endswith('.') or word.endswith('!') or word.endswith('?') or i == len(words) - 1:
            sentence = ' '.join(current_text)
            if len(sentence) > 20:  # 20ì ì´ìƒì¸ ë¬¸ì¥
                long_texts.append(sentence)
            current_text = []
    
    # ê°€ì¥ ê¸´ 3ê°œ ë¬¸ì¥ì„ í•µì‹¬ ë‚´ìš©ìœ¼ë¡œ
    structured_data["í•µì‹¬ë‚´ìš©"] = sorted(long_texts, key=len, reverse=True)[:3]
    
    # None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì œê±°
    structured_data = {k: v for k, v in structured_data.items() if v}
    
    return structured_data


# êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
def extract_structured_info(text, doc_type):
    structured_data = {}
    
    if doc_type == "ì˜ìˆ˜ì¦":
        # ë‚ ì§œ ì¶”ì¶œ
        date_pattern = r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})'
        date_match = re.search(date_pattern, text)
        if date_match:
            structured_data['date'] = f"{date_match.group(1)}.{date_match.group(2)}.{date_match.group(3)}"
        
        # ê¸ˆì•¡ ì¶”ì¶œ
        price_pattern = r'([0-9,]+)\s*ì›'
        prices = re.findall(price_pattern, text)
        if prices:
            structured_data['amounts'] = [p.replace(',', '') for p in prices]
            structured_data['total'] = max([int(p.replace(',', '')) for p in prices])
        
        # ìƒí˜¸ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        words = text.split()
        if len(words) > 0:
            structured_data['store'] = words[0]
    
    return structured_data

# í…ìŠ¤íŠ¸ ìš”ì•½
def summarize_text(text, tokenizer, model):
    if not text or len(text) < 50:
        return text
    
    inputs = tokenizer(text[:1024], return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(inputs["input_ids"], max_length=128, min_length=20, num_beams=4)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
def create_embedding(text, model):
    if not text or len(text.strip()) < 2:
        return None
    
    # ì§ì ‘ ë¬¸ì¥ ì„ë² ë”© ìƒì„± (í›¨ì”¬ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì )
    embedding = model.encode(text)
    return embedding.tolist()

# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(text, structured_data=None):
    stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ì—ì„œ', 'ìœ¼ë¡œ']
    words = text.split()
    keywords = [w for w in words if len(w) > 1 and w not in stopwords]
    
    # êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ
    if structured_data:
        if 'store' in structured_data:
            keywords.append(structured_data['store'])
        if 'date' in structured_data:
            keywords.append(structured_data['date'])
    
    return ", ".join(list(set(keywords)))

# ë¬¸ì„œ ì²˜ë¦¬
def process_document(uploaded_file, models, blur_size, block_size, C_value, dilation_iter):
    (dit_processor, dit_model, ocr, donut_processor, donut_model, 
     layout_processor, layout_model, sum_tokenizer, sum_model,
     embedding_model) = models
    
    image = Image.open(uploaded_file).convert("RGB")
    
    # 1. ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜
    doc_type = classify_document(image, dit_processor, dit_model)
    print('\n==========')
    print(f'\n[doc_type]\n{doc_type}')
    content, boxes = extract_text_with_layout(image, blur_size, block_size, C_value, dilation_iter)
    layoutlm_data = extract_structured_with_layoutlm(image, content, boxes, layout_processor, layout_model, doc_type)
    print('\n==========')
    print(f'\n[layoutlm_data]\n{layoutlm_data}')
    # 2. ë¬¸ì„œ ìœ í˜•ë³„ ì²˜ë¦¬
    if doc_type == "ì˜ìˆ˜ì¦":
        print('ì˜ìˆ˜ì¦')
        # Donutìœ¼ë¡œ ì˜ìˆ˜ì¦ ì •ë³´ ì¶”ì¶œ
        receipt_info = extract_receipt_info(image, donut_processor, donut_model)
        print('\n==========')
        print(f'\n[receipt_info]\n{receipt_info}')
        
        # êµ¬ì¡°í™”ëœ ì •ë³´ ë³‘í•©
        structured_data = extract_structured_info(content, doc_type)
        if receipt_info:
            # Donut ê²°ê³¼ê°€ ìˆìœ¼ë©´ LayoutMLê³¼ ë³‘í•©    
            for key, value in receipt_info.items():
                if key not in layoutlm_data or not layoutlm_data[key]:
                    layoutlm_data[key] = value
        else:
            structured_data = layoutlm_data
        structured_data.update(receipt_info)
    else:
        # ì¼ë°˜ OCR
        structured_data = layoutlm_data

    print('\n==========')
    print(f'\n[content]\n{content}')
    print('\n==========')
    print(f'\n[structured_data]\n{structured_data}')
    
    # 3. ìš”ì•½ ìƒì„±
    summary = summarize_text(content, sum_tokenizer, sum_model)

    print('\n==========')
    print(f'\n[summary]\n{summary}')
    
    # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(content, structured_data)
    print('\n==========')
    print(f'\n[keywords]\n{keywords}')
    
    # 5. ì„ë² ë”© ìƒì„±
    embedding = create_embedding(content + " " + summary, embedding_model)
    
    # ì´ë¯¸ì§€ ì €ì¥
    img_byte_arr = io.BytesIO()
    image.save(img_byte_arr, format='PNG')
    img_data = img_byte_arr.getvalue()
    
    return doc_type, content, summary, keywords, structured_data, img_data, embedding

# ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
def search_by_similarity(query, embedding_model, session):
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = create_embedding(query, embedding_model)
    
    # ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
    all_docs = session.exec(select(Document)).all()
    
    similarities = []
    for doc in all_docs:
        if doc.embedding:
            doc_embedding = json.loads(doc.embedding)
            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]
            similarities.append((doc, similarity))
    
    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    similarities.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, sim in similarities[:10] if sim > 0.5]

# ê²°ê³¼ ëª©ë¡ ì¶œë ¥
def print_result_list(results):
    for doc in results:
        with st.expander(f"{doc.filename} - {doc.upload_date.strftime('%Y-%m-%d %H:%M')}"):
            col1, col2 = st.columns(2)
            with col1:
                img = Image.open(io.BytesIO(doc.image_data))
                st.image(img, use_container_width=True)
            with col2:
                st.write(f"**ë¬¸ì„œ ìœ í˜•:** {doc.doc_type}")
                st.write(f"**ìš”ì•½:** {doc.summary}")
                st.write(f"**í‚¤ì›Œë“œ:** {doc.keywords}")
                
                if doc.structured_data:
                    structured = json.loads(doc.structured_data)
                    if structured:
                        st.write("**ì¶”ì¶œëœ ì •ë³´:**")
                        for k, v in structured.items():
                            st.write(f"- {k}: {v}")
                
                # ë‹¤ìš´ë¡œë“œ
                b64 = base64.b64encode(doc.image_data).decode()
                href = f'<a href="data:image/png;base64,{b64}" download="{doc.filename}">ë‹¤ìš´ë¡œë“œ</a>'
                st.markdown(href, unsafe_allow_html=True)

# Streamlit UI
st.title("AI ì•„ì¹´ì´ë¸Œ ì‹œìŠ¤í…œ")

# ëª¨ë¸ ë¡œë“œ
with st.spinner("AI ëª¨ë¸ ë¡œë”© ì¤‘..."):
    models = load_models()

# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["ë¬¸ì„œ ì—…ë¡œë“œ", "ë¬¸ì„œ ê²€ìƒ‰", "ë¬¸ì„œ ëª©ë¡"])

if 'processed_file' not in st.session_state:
    st.session_state.processed_file = None
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'doc_results' not in st.session_state:
    st.session_state.doc_results = None

# ë¬¸ì„œ ì—…ë¡œë“œ íƒ­
with tab1:
    uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['png', 'jpg', 'jpeg'])

    # ìƒˆ íŒŒì¼ì´ë©´ ì„¸ì…˜ ì´ˆê¸°í™” í•˜ê¸° + ì´ì „ ê²°ê³¼ ë°±ì—…í•˜ê¸°
    if uploaded_file is not None and uploaded_file != st.session_state.processed_file:
        # ì´ì „ ê²°ê³¼ ë°±ì—…
        if st.session_state.doc_results:
            st.session_state.prev_doc_results = st.session_state.doc_results.copy()

        st.session_state.processed_file = uploaded_file
        st.session_state.processing_complete = False
        st.session_state.ocr_ready = False    # OCRì€ ë²„íŠ¼ ëˆ„ë¥¼ ë•Œë§Œ ì§„í–‰í•œë‹¤.

    # íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆë‹¤ë©´
    if uploaded_file:
        st.subheader("ğŸ”§ ì „ì²˜ë¦¬ ì„¤ì •")

        blur_size = st.slider("ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì»¤ë„ í¬ê¸°", 1, 11, 5, step=2)
        block_size = st.slider("Adaptive Threshold blockSize", 3, 25, 11, step=2)
        C_value = st.slider("Threshold ìƒìˆ˜ C", 0, 10, 2)
        dilation_iter = st.slider("Dilation ë°˜ë³µ íšŸìˆ˜", 0, 3, 1)

        # ì „ì²˜ë¦¬ ë¯¸ë¦¬ë³´ê¸°
        pil_image = Image.open(uploaded_file).convert("RGB")
        final_image = preprocess_image_for_ocr(pil_image, blur_size, block_size, C_value, dilation_iter)

        st.image(final_image, caption="ì „ì²˜ë¦¬ ê²°ê³¼", channels="GRAY")

        if st.button("ğŸ”  OCR ì‹œì‘"):
            with st.spinner("OCR ë¶„ì„ ì¤‘..."):
                doc_type, content, summary, keywords, structured_data, img_data, embedding = process_document(
                    uploaded_file, models, blur_size, block_size, C_value, dilation_iter
                )
                st.session_state.processing_complete = True
                st.session_state.ocr_ready = True
                # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
                st.session_state.doc_results = {
                    'doc_type': doc_type,
                    'content': content,
                    'summary': summary,
                    'keywords': keywords,
                    'structured_data': structured_data,
                    'img_data': img_data,
                    'embedding': embedding
                }
        
    prev_results = st.session_state.get("prev_doc_results")
    
    # ì²˜ë¦¬ ì™„ë£Œëœ ê²°ê³¼ í‘œì‹œ
    if uploaded_file is not None and st.session_state.doc_results is not None:
        results = st.session_state.doc_results
        

        st.subheader("ğŸ“„ í˜„ì¬ ë¶„ì„ëœ ë¬¸ì„œ ê²°ê³¼")
        
        col1, col2 = st.columns(2)
        with col1:
            st.image(uploaded_file, caption="ì—…ë¡œë“œëœ ë¬¸ì„œ", use_container_width=True)
        
        with col2:
            st.write(f"**ë¬¸ì„œ ìœ í˜•:** {results['doc_type']}")
            st.write(f"**ìš”ì•½:** {results['summary']}")
            st.write(f"**í‚¤ì›Œë“œ:** {results['keywords']}")
            
            if results['structured_data']:
                st.write("**ì¶”ì¶œëœ ì •ë³´:**")
                for key, value in results['structured_data'].items():
                    st.write(f"- {key}: {value}")
            
            if st.button("ì €ì¥"):
                with Session(engine) as session:
                    doc = Document(
                        filename=uploaded_file.name,
                        doc_type=results['doc_type'],
                        content=results['content'],
                        summary=results['summary'],
                        keywords=results['keywords'],
                        structured_data=json.dumps(results['structured_data'], ensure_ascii=False),
                        image_data=results['img_data'],
                        embedding=json.dumps(results['embedding'])
                    )
                    session.add(doc)
                    session.commit()
                st.success("ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.session_state.processed_file = None
                st.session_state.processing_complete = False
                st.session_state.doc_results = None

    # ì´ì „ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë¹„êµí•˜ê¸°
    if prev_results:
        st.markdown("---")
        st.subheader("ğŸ” ì´ì „ ë¶„ì„ ê²°ê³¼ì™€ ë¹„êµ")

        col1, col2 = st.columns(2)

        with col1:
            st.write("ğŸ“„ **ì´ì „ ìš”ì•½**")
            st.write(prev_results["summary"])
            st.write("ğŸ”‘ **ì´ì „ í‚¤ì›Œë“œ**")
            st.write(prev_results["keywords"])

            if prev_results.get("structured_data"):
                st.write("ğŸ“‹ **ì´ì „ ì¶”ì¶œ ì •ë³´**")
                for key, value in prev_results["structured_data"].items():
                    st.write(f"- {key}: {value}")

        with col2:
            st.write("ğŸ“„ **í˜„ì¬ ìš”ì•½**")
            st.write(results["summary"])
            st.write("ğŸ”‘ **í˜„ì¬ í‚¤ì›Œë“œ**")
            st.write(results["keywords"])

            if results.get("structured_data"):
                st.write("ğŸ“‹ **í˜„ì¬ ì¶”ì¶œ ì •ë³´**")
                for key, value in results["structured_data"].items():
                    st.write(f"- {key}: {value}")

# ë¬¸ì„œ ê²€ìƒ‰ íƒ­
with tab2:
    search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì»¤í”¼ ì˜ìˆ˜ì¦)")
    search_method = st.radio("ê²€ìƒ‰ ë°©ë²•", ["ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰", "í‚¤ì›Œë“œ ê²€ìƒ‰"])
    
    if st.button("ê²€ìƒ‰", key='search_button'):
        with Session(engine) as session:
            if search_method == "ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰":
                results = search_by_similarity(
                    search_query, 
                    models[9],  # embedding_model
                    session
                )
            else:
                # í‚¤ì›Œë“œ ê²€ìƒ‰
                statement = select(Document).where(
                    Document.keywords.contains(search_query) | 
                    Document.summary.contains(search_query) |
                    Document.doc_type.contains(search_query)
                )
                results = session.exec(statement).all()
            
            if results:
                print_result_list(results)
            else:
                st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ë¬¸ì„œ ëª©ë¡ íƒ­
with tab3:
    with Session(engine) as session:
        statement = select(Document)
        results = session.exec(statement).all()
        if results:
            print_result_list(results)
    