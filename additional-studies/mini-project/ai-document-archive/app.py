import streamlit as st
import torch
from sentence_transformers import SentenceTransformer
from transformers import (
    AutoModelForImageClassification, AutoProcessor, 
    AutoTokenizer, AutoModelForSeq2SeqLM,
    VisionEncoderDecoderModel, LayoutLMv3Processor, LayoutLMv3ForTokenClassification,
)
from konlpy.tag import Komoran
from paddleocr import PaddleOCR
from sqlmodel import Field, Session, SQLModel, create_engine, select
from datetime import datetime
from PIL import Image
import piexif
import cv2
import io
import base64
import numpy as np
from typing import Optional
import json
import re
from datetime import datetime
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
from geopy.geocoders import Nominatim

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
class Document(SQLModel, table=True):
    __table_args__ = {'extend_existing': True}
    id: Optional[int] = Field(default=None, primary_key=True)
    filename: str
    doc_type: str
    content: str
    summary: str
    keywords: str
    structured_data: str  # JSON í˜•íƒœë¡œ ì €ì¥
    upload_date: datetime = Field(default_factory=datetime.now)
    image_data: bytes
    embedding: Optional[str] = None  # ë²¡í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
engine = create_engine("sqlite:///archive.db")
SQLModel.metadata.create_all(engine)

komoran = Komoran()

# ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_models():
    # DiT ë¬¸ì„œ ë¶„ë¥˜
    dit_processor = AutoProcessor.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    dit_model = AutoModelForImageClassification.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    
    # OCR
    ocr = PaddleOCR(lang='korean')
    
    # Donut (ì˜ìˆ˜ì¦ ì „ìš©)
    donut_processor = AutoProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
    donut_model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
    
    # LayoutLMv3
    layout_processor = LayoutLMv3Processor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
    layout_model = LayoutLMv3ForTokenClassification.from_pretrained("microsoft/layoutlmv3-base")
    
    # í…ìŠ¤íŠ¸ ìš”ì•½
    summarizer_tokenizer = AutoTokenizer.from_pretrained("gangyeolkim/kobart-korean-summarizer-v2")
    summarizer_model = AutoModelForSeq2SeqLM.from_pretrained("gangyeolkim/kobart-korean-summarizer-v2")
    
    # ì„ë² ë”© ëª¨ë¸ (ë²¡í„° ê²€ìƒ‰ìš©)
    embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
    
    return (dit_processor, dit_model, ocr, donut_processor, donut_model, 
            layout_processor, layout_model, summarizer_tokenizer, summarizer_model,
            embedding_model)

# ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_yolo_model():
    # Yolo ëª¨ë¸ êµ¬ì„± íŒŒì¼ê³¼ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ
    config_path = "yolov3.cfg"
    weights_path = "yolov3.weights"
    names_path = "coco.names"  # í´ë˜ìŠ¤ ì´ë¦„ íŒŒì¼

    # ëª¨ë¸ ë¡œë“œ
    net = cv2.dnn.readNet(weights_path, config_path)

    # í´ë˜ìŠ¤ ì´ë¦„ë“¤ ë¡œë“œ
    with open(names_path, 'r') as f:
        classes = f.read().strip().split("\n")
    
    return net, classes

# ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜
def classify_document(image, dit_processor, dit_model):
    inputs = dit_processor(images=image, return_tensors="pt")
    outputs = dit_model(**inputs)
    predicted_class_idx = outputs.logits.argmax(-1).item()
    predicted_class = dit_model.config.id2label[predicted_class_idx]
    
    # ì˜ìˆ˜ì¦ ê´€ë ¨ í´ë˜ìŠ¤ ë§¤í•‘
    if any(keyword in predicted_class.lower() for keyword in ['invoice', 'receipt']):
        return "ì˜ìˆ˜ì¦"
    
    # ë¬¸ì„œ ê´€ë ¨ í´ë˜ìŠ¤ê°€ ì•„ë‹ˆë©´ "ì¼ë°˜ ì‚¬ì§„"ìœ¼ë¡œ ë°˜í™˜
    if any(keyword in predicted_class.lower() for keyword in ['image', 'photo', 'landscape', 'nature']):
        return "ì¼ë°˜ ì‚¬ì§„"
    
    return predicted_class

# ì˜ìˆ˜ì¦ OCR (Donut)
def extract_receipt_info(image, processor, model):
    pixel_values = processor(image, return_tensors="pt").pixel_values
    decoder_input_ids = processor.tokenizer("<s_cord-v2>", return_tensors="pt").input_ids
    
    outputs = model.generate(pixel_values, decoder_input_ids=decoder_input_ids, max_length=512)
    prediction = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    
    # JSON íŒŒì‹±
    try:
        receipt_data = json.loads(prediction)
        return receipt_data
    except:
        return {}


# ê·¸ë ˆì´ ìŠ¤ì¼€ì¼ ë³€í™˜í•˜ê¸°
def convert_to_grayscale(img_cv):
    gray_image = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
    return gray_image

# ë…¸ì´ì¦ˆ ì œê±°í•˜ê¸°
def remove_noise(gray_image, blur_size):
    # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì‚¬ìš© => ì»¤ë„ì˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ë” ë¶€ë“œëŸ½ê²Œ ëœë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ í¬ë©´ ë­‰ê°œì§.
    # ì–‡ì€ ê¸€ìì¼ ê²½ìš° -> ì‘ê²Œ / ì‰í¬ ë²ˆì§ê³¼ ê°™ì€ ë…¸ì´ì¦ˆê°€ ë§ì„ ê²½ìš° -> ì¡°ê¸ˆ ë” í¬ê²Œ
    denoised = cv2.GaussianBlur(gray_image, (blur_size, blur_size), 0)
    return denoised

# ëŒ€ë¹„ ê°œì„ í•˜ê¸°
def improve_contrast(denoised):
    # ì „ì²´ì ì¸ ëŒ€ë¹„ í–¥ìƒ -> ì¡°ëª…ì— ë”°ë¼ ë¶€ìì—°ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆìŒ.
    # enhanced = cv2.equalizeHist(denoised)
    # ë” ì¢‹ì€ ëŒ€ì•ˆ : CLAHE (ì ì‘ì  íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”)
    # clipLimitê°€ ë‚®ì„ìˆ˜ë¡ ë¶€ë“œëŸ½ê³ , ë†’ì„ìˆ˜ë¡ ëŒ€ë¹„ê°€ ê°•ì¡°ëœë‹¤.
    # tileGridSizeëŠ” ê¸€ì í¬ê¸° ê¸°ì¤€ ë³´í†µ (8, 8), ê¸€ìê°€ ì‘ìœ¼ë©´ (4, 4)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(denoised) 
    return enhanced

# ì´ì§„í™”
def apply_adaptive_binarization(enhanced, block_size, C_value):
    binary = cv2.adaptiveThreshold(
        enhanced, 255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2. THRESH_BINARY,
        block_size,     # ì£¼ë³€ ë¸”ë¡ í¬ê¸°. ê¸€ì í¬ê¸°ë³´ë‹¤ ì¡°ê¸ˆ í° ê°’ì´ ì¢‹ìŒ.
        C_value       # ë¹¼ëŠ” ìƒìˆ˜. ì–´ë‘ìš´ ë°°ê²½ì¼ìˆ˜ë¡ ì¡°ê¸ˆ ë” í¬ê²Œ ì¡°ì ˆ ê°€ëŠ¥.
    )
    return binary

# í…ìŠ¤íŠ¸ ì˜ì—­ ê°•í™”
def enhance_text_regions(binary, dilation_iter):
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    # í…ìŠ¤íŠ¸ êµµê²Œ í•´ì„œ OCRì´ ì˜ ë˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤.
    # ì–‡ì€ ê¸€ì”¨ : iterations=1
    # ëŠê¸´ ê¸€ì”¨ë‚˜ ë²ˆì§„ ì‰í¬ëŠ” (3, 3)ë³´ë‹¤ í° ì»¤ë„ë¡œ iterations=2~3ë„ ì‹¤í—˜í•´ë³¼ë§Œí•˜ë‹¤.
    final_image = cv2.dilate(binary, kernel, iterations=dilation_iter)
    return final_image


# OCR ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
def preprocess_image_for_ocr(image, blur_size, block_size, C_value, dilation_iter, to_grayscale):
    """OCR ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    # PILì„ openCVë¡œ ë³€í™˜
    img_np = np.array(image)  # Pillow â†’ numpy
    img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

    # 1. ê·¸ë ˆì´ ìŠ¤ì¼€ì¼ ë³€í™”
    if to_grayscale:
        gray_image = convert_to_grayscale(img_cv)
    else:
        gray_image = img_cv

    # 2. ë…¸ì´ì¦ˆ ì œê±°
    denoised = remove_noise(gray_image, blur_size)

    # 3. ëŒ€ë¹„ ê°œì„ 
    if to_grayscale:
        enhanced = improve_contrast(denoised)
    else:
        enhanced = denoised

    # 4. ì´ì§„í™” (í…ìŠ¤íŠ¸ì™€ ë°°ê²½ ë¶„ë¦¬)
    if to_grayscale:
        binary = apply_adaptive_binarization(enhanced, block_size, C_value)
    else:
        binary = enhanced

    # 5. í…ìŠ¤íŠ¸ ì˜ì—­ ê°•í™”
    final_image = enhance_text_regions(binary, dilation_iter)

    return final_image


# OCR ìˆ˜í–‰
def perform_ocr(image, ocr):
    result = ocr.ocr(np.array(image))
    return result


# í…ìŠ¤íŠ¸ì™€ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
def extract_text_and_positions(result):
    text = ""
    boxes = []
    
    if result[0]:
        for line in result[0]:
            text += line[1][0] + " "
            boxes.append(line[0])

    return text.strip(), boxes
    

# ê¸°ì¡´ OCR í•¨ìˆ˜ì— ì „ì²˜ë¦¬ ì˜µì…˜ ì¶”ê°€í•˜ê¸°
def extract_text_with_layout(ocr, image, blur_size, block_size, C_value, dilation_iter, to_grayscale, use_preprocessing=True):
    """ì „ì²˜ë¦¬ ì˜µì…˜ì´ ì¶”ê°€ëœ OCR í•¨ìˆ˜"""
    # ì „ì²˜ë¦¬ ì ìš© ì—¬ë¶€ í™•ì¸í•˜ê¸°
    if use_preprocessing:
        image = preprocess_image_for_ocr(image, blur_size, block_size, C_value, dilation_iter, to_grayscale)

    # OCR ìˆ˜í–‰í•˜ê¸°
    result = perform_ocr(image, ocr)

    # í…ìŠ¤íŠ¸ì™€ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œí•˜ê¸°
    text, boxes = extract_text_and_positions(result)
    
    return text, boxes

#---------------------------------------------------------------------------------

# LayoutLMv3ë¥¼ í™œìš©í•œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜
def extract_structured_with_layoutlm(image, text, boxes, layout_processor, layout_model, doc_type):
    """
    LayoutLMv3ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì£¼ì˜: ê¸°ë³¸ LayoutLMv3ëŠ” íŠ¹ì • íƒœìŠ¤í¬ì— fine-tuningë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ,
    ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì •ë³´ë¥¼ í™œìš©í•œ íœ´ë¦¬ìŠ¤í‹± ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        image: PIL Image ê°ì²´
        text: OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸
        boxes: OCRë¡œ ì¶”ì¶œí•œ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
        layout_processor: LayoutLMv3 í”„ë¡œì„¸ì„œ
        layout_model: LayoutLMv3 ëª¨ë¸
        doc_type: ë¬¸ì„œ ìœ í˜• (ì˜ìˆ˜ì¦, ë¬¸ì„œ ë“±)
    
    Returns:
        structured_data: ì¶”ì¶œëœ êµ¬ì¡°í™”ëœ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    # OCR ê²°ê³¼ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    words = text.split()
    
    # ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ ë‹¨ì–´ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    if not words:
        return {}
    
    # ë°”ìš´ë”© ë°•ìŠ¤ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•œ ê²½ìš° ì²˜ë¦¬
    if not boxes or len(boxes) == 0:
        # ë°•ìŠ¤ ì •ë³´ ì—†ì´ ê¸°ë³¸ êµ¬ì¡°ë§Œ ë°˜í™˜
        if doc_type == "ì˜ìˆ˜ì¦":
            return {"í…ìŠ¤íŠ¸": text}
        else:
            return {"ë‚´ìš©": text}
    
    # ë°”ìš´ë”© ë°•ìŠ¤ê°€ ë‹¨ì–´ ìˆ˜ë³´ë‹¤ ì ì„ ê²½ìš° ì²˜ë¦¬
    if len(boxes) < len(words):
        # ë§ˆì§€ë§‰ ë°•ìŠ¤ë¥¼ ë³µì‚¬í•˜ì—¬ ë§ì¶¤
        boxes = boxes + [boxes[-1]] * (len(words) - len(boxes))
    
    # ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì •ê·œí™” (0-1000 ë²”ìœ„ë¡œ)
    width, height = image.size
    normalized_boxes = []
    word_positions = []  # ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ ì €ì¥
    
    for idx, box in enumerate(boxes[:len(words)]):
        # boxëŠ” [[x1,y1], [x2,y1], [x2,y2], [x1,y2]] í˜•íƒœ
        if len(box) >= 4:
            x1, y1 = box[0]
            x2, y2 = box[2]
            # LayoutLMì€ [x1, y1, x2, y2] í˜•íƒœë¡œ 0-1000 ë²”ìœ„ ì¢Œí‘œ í•„ìš”
            norm_box = [
                int(x1 * 1000 / width),
                int(y1 * 1000 / height),
                int(x2 * 1000 / width),
                int(y2 * 1000 / height)
            ]
            normalized_boxes.append(norm_box)
            
            # ìœ„ì¹˜ ì •ë³´ ì €ì¥ (ìƒëŒ€ì  ìœ„ì¹˜)
            word_positions.append({
                'word': words[idx] if idx < len(words) else '',
                'x_center': (norm_box[0] + norm_box[2]) / 2,
                'y_center': (norm_box[1] + norm_box[3]) / 2,
                'width': norm_box[2] - norm_box[0],
                'height': norm_box[3] - norm_box[1],
                'area': (norm_box[2] - norm_box[0]) * (norm_box[3] - norm_box[1])
            })
    
    # LayoutLMv3 ì…ë ¥ ì¤€ë¹„
    encoding = layout_processor(
        image,
        words[:len(normalized_boxes)],
        boxes=normalized_boxes,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=512
    )
    
    # ëª¨ë¸ ì¶”ë¡  - ì„ë² ë”© ì¶”ì¶œ
    with torch.no_grad():
        outputs = layout_model(**encoding, output_hidden_states=True)
    
    # ë§ˆì§€ë§‰ ì€ë‹‰ì¸µì˜ ì„ë² ë”© ì‚¬ìš©
    last_hidden_states = outputs.hidden_states[-1]
    
    # ë¬¸ì„œ ìœ í˜•ë³„ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
    if doc_type == "ì˜ìˆ˜ì¦":
        structured_data = extract_receipt_structure(word_positions, words, last_hidden_states)
    else:
        structured_data = extract_document_structure(word_positions, words, last_hidden_states)
    
    return structured_data


def extract_receipt_structure(word_positions, words, embeddings):
    """
    ì˜ìˆ˜ì¦ ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ìœ„ì¹˜ ì •ë³´ì™€ í…ìŠ¤íŠ¸ íŒ¨í„´ì„ í™œìš©í•œ íœ´ë¦¬ìŠ¤í‹± ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    structured_data = {
        "ìƒí˜¸ëª…": None,
        "ë‚ ì§œ": None,
        "ì‹œê°„": None,
        "í’ˆëª©": [],
        "ê¸ˆì•¡": [],
        "í•©ê³„": None,
        "ì£¼ì†Œ": None,
        "ì „í™”ë²ˆí˜¸": None,
        "ì‚¬ì—…ìë²ˆí˜¸": None
    }
    
    # ìœ„ì¹˜ë³„ë¡œ ë‹¨ì–´ ê·¸ë£¹í™”
    # ìƒë‹¨ 20% - ì£¼ë¡œ ìƒí˜¸ëª…, ì£¼ì†Œ
    # ì¤‘ê°„ 60% - í’ˆëª©ê³¼ ê¸ˆì•¡
    # í•˜ë‹¨ 20% - í•©ê³„, ì‚¬ì—…ìë²ˆí˜¸ ë“±
    
    top_words = []
    middle_words = []
    bottom_words = []
    
    for pos in word_positions:
        if pos['y_center'] < 200:  # ìƒë‹¨
            top_words.append(pos)
        elif pos['y_center'] < 800:  # ì¤‘ê°„
            middle_words.append(pos)
        else:  # í•˜ë‹¨
            bottom_words.append(pos)
    
    # 1. ìƒí˜¸ëª… ì¶”ì¶œ (ìƒë‹¨ì—ì„œ ê°€ì¥ í° í…ìŠ¤íŠ¸)
    if top_words:
        # ë©´ì ì´ ê°€ì¥ í° í…ìŠ¤íŠ¸ë¥¼ ìƒí˜¸ëª…ìœ¼ë¡œ ì¶”ì •
        largest = max(top_words, key=lambda x: x['area'])
        structured_data["ìƒí˜¸ëª…"] = largest['word']
    
    # 2. ë‚ ì§œì™€ ì‹œê°„ ì¶”ì¶œ
    for pos in word_positions:
        word = pos['word']
        # ë‚ ì§œ íŒ¨í„´
        date_match = re.search(r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})', word)
        if date_match:
            structured_data["ë‚ ì§œ"] = word
        
        # ì‹œê°„ íŒ¨í„´
        time_match = re.search(r'(\d{1,2}):(\d{2})', word)
        if time_match:
            structured_data["ì‹œê°„"] = word
    
    # 3. ê¸ˆì•¡ ì¶”ì¶œ (ì¤‘ê°„ ì˜ì—­ì—ì„œ ìš°ì¸¡ì— ìœ„ì¹˜í•œ ìˆ«ì)
    amount_pattern = r'[\d,]+ì›?'
    for pos in middle_words:
        word = pos['word']
        if re.match(amount_pattern, word) and pos['x_center'] > 500:  # ìš°ì¸¡
            # ìˆ«ìë§Œ ì¶”ì¶œ (ì½¤ë§ˆ, ì›, ê¸°íƒ€ ë¬¸ì ì œê±°)
            cleaned_amount = re.sub(r'[^\d]', '', word)
            if cleaned_amount and cleaned_amount.isdigit():
                structured_data["ê¸ˆì•¡"].append(cleaned_amount)
    
    # 4. í’ˆëª© ì¶”ì¶œ (ì¤‘ê°„ ì˜ì—­ì—ì„œ ì¢Œì¸¡ì— ìœ„ì¹˜í•œ í…ìŠ¤íŠ¸)
    for pos in middle_words:
        word = pos['word']
        if pos['x_center'] < 500 and not re.match(r'[\d,]+', word):  # ì¢Œì¸¡, ìˆ«ì ì•„ë‹˜
            structured_data["í’ˆëª©"].append(word)
    
    # 5. í•©ê³„ ì¶”ì¶œ (í•˜ë‹¨ì—ì„œ ê°€ì¥ í° ê¸ˆì•¡)
    total_candidates = []
    for pos in bottom_words:
        word = pos['word']
        if 'í•©ê³„' in word or 'ì´' in word:
            # í•©ê³„ í‚¤ì›Œë“œ ê·¼ì²˜ì˜ ê¸ˆì•¡ ì°¾ê¸°
            idx = word_positions.index(pos)
            for i in range(max(0, idx-3), min(len(word_positions), idx+4)):
                nearby_word = word_positions[i]['word']
                if re.match(r'[\d,]+ì›?', nearby_word):
                    # ìˆ«ìë§Œ ì¶”ì¶œ
                    cleaned_amount = re.sub(r'[^\d]', '', nearby_word)
                    if cleaned_amount and cleaned_amount.isdigit():
                        amount = int(cleaned_amount)
                        total_candidates.append(amount)
    
    if total_candidates:
        structured_data["í•©ê³„"] = str(max(total_candidates))
    elif structured_data["ê¸ˆì•¡"]:
        # í•©ê³„ê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš° ê¸ˆì•¡ ì¤‘ ìµœëŒ€ê°’
        try:
            amounts = []
            for x in structured_data["ê¸ˆì•¡"]:
                if x.isdigit():
                    amounts.append(int(x))
            if amounts:
                structured_data["í•©ê³„"] = str(max(amounts))
        except (ValueError, AttributeError):
            pass  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ í•©ê³„ ìƒëµ
    
    # 6. ì‚¬ì—…ìë²ˆí˜¸ ì¶”ì¶œ
    for word in words:
        # ì‚¬ì—…ìë²ˆí˜¸ íŒ¨í„´ (XXX-XX-XXXXX)
        business_match = re.search(r'\d{3}-\d{2}-\d{5}', word)
        if business_match:
            structured_data["ì‚¬ì—…ìë²ˆí˜¸"] = business_match.group()
    
    # 7. ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
    for word in words:
        # ì „í™”ë²ˆí˜¸ íŒ¨í„´
        phone_match = re.search(r'(\d{2,3})-(\d{3,4})-(\d{4})', word)
        if phone_match:
            structured_data["ì „í™”ë²ˆí˜¸"] = phone_match.group()
    
    # None ê°’ ì œê±°
    structured_data = {k: v for k, v in structured_data.items() if v}
    
    return structured_data


def extract_document_structure(word_positions, words, embeddings):
    """
    ì¼ë°˜ ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    structured_data = {
        "ì œëª©": None,
        "ë¶€ì œëª©": [],
        "ë‚ ì§œ": None,
        "ì‘ì„±ì": None,
        "í•µì‹¬ë‚´ìš©": [],
        "ë²ˆí˜¸ì •ë³´": []  # ì „í™”ë²ˆí˜¸, ê³„ì¢Œë²ˆí˜¸ ë“±
    }
    
    # 1. ì œëª© ì¶”ì¶œ (ìƒë‹¨ì—ì„œ ê°€ì¥ í° í…ìŠ¤íŠ¸)
    top_words = [pos for pos in word_positions if pos['y_center'] < 200]
    if top_words:
        largest = max(top_words, key=lambda x: x['area'])
        structured_data["ì œëª©"] = largest['word']
    
    # 2. ë¶€ì œëª© ì¶”ì¶œ (ì¤‘ê°„ í¬ê¸°ì˜ í…ìŠ¤íŠ¸)
    avg_area = sum(pos['area'] for pos in word_positions) / len(word_positions)
    for pos in word_positions:
        if pos['area'] > avg_area * 1.5 and pos['word'] != structured_data.get("ì œëª©"):
            structured_data["ë¶€ì œëª©"].append(pos['word'])
    
    # 3. ë‚ ì§œ ì¶”ì¶œ
    for word in words:
        date_match = re.search(r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})', word)
        if date_match:
            structured_data["ë‚ ì§œ"] = date_match.group()
            break
    
    # 4. ë²ˆí˜¸ ì •ë³´ ì¶”ì¶œ
    for word in words:
        # ì „í™”ë²ˆí˜¸
        phone_match = re.search(r'(\d{2,3})-(\d{3,4})-(\d{4})', word)
        if phone_match:
            structured_data["ë²ˆí˜¸ì •ë³´"].append(f"ì „í™”: {phone_match.group()}")
        
        # ê³„ì¢Œë²ˆí˜¸ íŒ¨í„´ (ìˆ«ìê°€ 10ìë¦¬ ì´ìƒ)
        account_match = re.search(r'\d{10,}', word)
        if account_match:
            structured_data["ë²ˆí˜¸ì •ë³´"].append(f"ê³„ì¢Œ: {account_match.group()}")
    
    # 5. í•µì‹¬ ë‚´ìš© ì¶”ì¶œ (í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ê¸´ ë¶€ë¶„)
    long_texts = []
    current_text = []
    
    for i, word in enumerate(words):
        current_text.append(word)
        # ë¬¸ì¥ ì¢…ë£Œ íŒë‹¨
        if word.endswith('.') or word.endswith('!') or word.endswith('?') or i == len(words) - 1:
            sentence = ' '.join(current_text)
            if len(sentence) > 20:  # 20ì ì´ìƒì¸ ë¬¸ì¥
                long_texts.append(sentence)
            current_text = []
    
    # ê°€ì¥ ê¸´ 3ê°œ ë¬¸ì¥ì„ í•µì‹¬ ë‚´ìš©ìœ¼ë¡œ
    structured_data["í•µì‹¬ë‚´ìš©"] = sorted(long_texts, key=len, reverse=True)[:3]
    
    # None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì œê±°
    structured_data = {k: v for k, v in structured_data.items() if v}
    
    return structured_data


# êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
def extract_structured_info(text, doc_type):
    structured_data = {}
    
    if doc_type == "ì˜ìˆ˜ì¦":
        # ë‚ ì§œ ì¶”ì¶œ
        date_pattern = r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})'
        date_match = re.search(date_pattern, text)
        if date_match:
            structured_data['date'] = f"{date_match.group(1)}.{date_match.group(2)}.{date_match.group(3)}"
        
        # ê¸ˆì•¡ ì¶”ì¶œ
        price_pattern = r'([0-9,]+)\s*ì›'
        prices = re.findall(price_pattern, text)
        if prices:
            structured_data['amounts'] = [p.replace(',', '') for p in prices]
            structured_data['total'] = max([int(p.replace(',', '')) for p in prices])
        
        # ìƒí˜¸ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        words = text.split()
        if len(words) > 0:
            structured_data['store'] = words[0]
    
    return structured_data

# í…ìŠ¤íŠ¸ ìš”ì•½
def summarize_text(text, tokenizer, model):
    if not text or len(text) < 50:
        return text
    
    inputs = tokenizer(text[:1024], return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(inputs["input_ids"], max_length=128, min_length=20, num_beams=4)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
def create_embedding(text, model):
    if not text or len(text.strip()) < 2:
        return None
    
    # ì§ì ‘ ë¬¸ì¥ ì„ë² ë”© ìƒì„± (í›¨ì”¬ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì )
    embedding = model.encode(text)
    return embedding.tolist()

# í˜•íƒœì†Œ ë¶„ì„ ë° í’ˆì‚¬ íƒœê¹…í•˜ê¸°
def morpheme_analyze(text):
    pos_tagged = komoran.pos(text)
    return pos_tagged
    
# ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_nouns_from_pos(pos_tagged):
    nouns = [word for word, tag in pos_tagged if tag == "Noun" and len(word) > 1]
    return list(set(nouns))

# ë¶ˆìš©ì–´ í•„í„°ë§ í•¨ìˆ˜
def filter_stopwords(nouns, stopwords=None):
    if stopwords is None:
        stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ì—ì„œ', 'ìœ¼ë¡œ']

    filtered = [word for word in nouns if word not in stopwords]
    return filtered

# ë³µí•© ëª…ì‚¬ ìƒì„±
def create_compound_nouns(pos_tagged):

    compound_nouns = []
    temp = []

    # íƒœê¹…ëœ ë‹¨ì–´ë“¤ì„ ìˆœíšŒí•œë‹¤.
    for word, tag in pos_tagged:
        if tag in ["NNG", "NNP"]:    # ì¼ë°˜ ëª…ì‚¬, ê³ ìœ  ëª…ì‚¬
            temp.append(word)

        # ëª…ì‚¬ íƒœê·¸ê°€ ì—†ë‹¤ë©´
        else:
            if len(temp) > 1:
                compound_nouns.append(''.join(temp))
            temp = []

    if len(temp) > 1:
        compound_nouns.append(''.join(temp))

    return compound_nouns

# TF-IDF ì ìˆ˜ë¥¼ í†µí•œ í‚¤ì›Œë“œ ì¤‘ìš”ë„ ê³„ì‚°í•˜ê¸°
def calculate_tfidf_scores(current_nouns, type_grouped_nouns, threshold=3):
    """
    current_nouns: í˜„ì¬ ë¬¸ì„œì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
    type_grouped_nouns: ë™ì¼ ìœ í˜•ì˜ ì „ì²´ ë¬¸ì„œë“¤ì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
    threshold: TF-IDFë¥¼ ì ìš©í•  ìµœì†Œ ë¬¸ì„œì˜ ìˆ˜
    """
    if len(type_grouped_nouns) >= threshold:
        # TF-IDF ì ìš©í•˜ê¸°
        all_docs = type_grouped_nouns + [current_nouns]
        docs = [' '.join(doc) for doc in all_docs]

        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(docs)

        feature_names = vectorizer.get_feature_names_out()
        scores = tfidf_matrix.toarray()[-1]    # ë§ˆì§€ë§‰ì´ í˜„ì¬ì˜ ë¬¸ì„œì´ê¸° ë•Œë¬¸ì—

        word_scores = dict(zip(feature_names, scores))
        method = "tfidf"

    else:
        # ë‹¨ì–´ ë¹ˆë„ ì ìš©
        word_scores = dict(Counter(current_nouns))
        method = "term-frequency"
    
    return word_scores, method

# ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
def select_top_keywords(word_scores, top_k):
    # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_keywords = sorted(word_scores.items(), key=lambda x:x[1], reverse=True)

    # ìƒìœ„ top_kë§Œ ì„ íƒí•˜ê¸°
    top_keywords = []
    for word, score in sorted_keywords[:top_k]:
        top_keywords.append(word)
    
    return top_keywords

# í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords_with_morpheme_analysis(text, top_k=15):
    """í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # 1. í˜•íƒœì†Œ ë¶„ì„ ë° í’ˆì‚¬ íƒœê¹…
    pos_tagged = morpheme_analyze(text)

    # 2. ëª…ì‚¬ë§Œ ì¶”ì¶œ (ì¼ë°˜ ëª…ì‚¬, ê³ ìœ  ëª…ì‚¬)
    nouns = extract_nouns_from_pos(pos_tagged)

    # ë¶ˆìš©ì–´ í•„í„°ë§
    filtered_nouns = filter_stopwords(nouns)

    # 3. ë³µí•© ëª…ì‚¬ ìƒì„±
    compound_nouns = create_compound_nouns(pos_tagged)

    # ë³µí•© ëª…ì‚¬ + ì¼ë°˜ ëª…ì‚¬ + ê³ ìœ  ëª…ì‚¬
    filtered_nouns.extend(compound_nouns)

    # 4. TF-IDF ì ìˆ˜ ê³„ì‚°
    # ìš°ì„  ê°™ì€ ìœ í˜•ì˜ ë¬¸ì„œê°€ ì—†ë‹¤ê³  ê°€ì •í•œë‹¤.
    word_scores, method = calculate_tfidf_scores(filtered_nouns, [])

    # 5. ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
    top_keywords = select_top_keywords(word_scores, top_k)

    return top_keywords, method

# êµ¬ì¡°í™”ëœ ë°ì´í„°(ex. ì˜ìˆ˜ì¦)ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ê¸° í•¨ìˆ˜
def extract_structured_keywords(structured_data):
    structured_data_keywords = []

    if 'store' in structured_data:
        structured_data_keywords.append(structured_data['store'])
    if 'date' in structured_data:
        structured_data_keywords.append(structured_data['date'])

    return structured_data_keywords

# ê¸°ì¡´ í•¨ìˆ˜ ëŒ€ì²´í•˜ê¸°
def extract_keywords(text, structured_data=None):
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜"""
    # í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords, method = extract_keywords_with_morpheme_analysis(text)
    
    # êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ
    if structured_data:
        keywords.extend(extract_structured_keywords(structured_data))
    
    return ", ".join(list(set(keywords)))

#---------------------------------------------------------------------------
# ì‚¬ì§„ êµ¬ë¶„ ë° ë©”íƒ€ë°ì´í„° ê²€ìƒ‰
# EXIF ë°ì´í„° ì½ê¸°
def read_exif_data(image):
    # EXIF ë°ì´í„° ì¶”ì¶œ
    exif_info = piexif.load(image.info["exif"])

    return exif_info

# ì£¼ìš” ì •ë³´ ì¶”ì¶œí•˜ê¸°
def extract_key_info(exif_info):
    metadata = {}

    # ì´¬ì˜ ì¼ì
    date_time = exif_info.get("0th", {}).get(piexif.ImageIFD.DateTime, None)
    if date_time:
        metadata["date_time"] = date_time.decode("utf-8")

    # ì¹´ë©”ë¼ ì œì¡°ì‚¬
    make = exif_info.get("0th", {}).get(piexif.ImageIFD.Make, None)
    if make:
        metadata["make"] = make.decode("utf-8")

    # ì¹´ë©”ë¼ ëª¨ë¸
    model = exif_info.get("0th", {}).get(piexif.ImageIFD.Model, None)
    if model:
        metadata["model"] = model.decode("utf-8")

    # GPS ì¢Œí‘œ (ìœ„ë„/ê²½ë„)
    # 4. GPS ì¢Œí‘œ (ìœ„ë„/ê²½ë„)
    gps_info = exif_info.get("GPS", {})
    latitude = gps_info.get(piexif.GPSIFD.GPSLatitude, None)
    longitude = gps_info.get(piexif.GPSIFD.GPSLongitude, None)

    if latitude and longitude:
        # ìœ„ë„/ê²½ë„ ê³„ì‚°
        lat_degree = latitude[0][0] / latitude[0][1]
        lat_minute = latitude[1][0] / latitude[1][1]
        lat_second = latitude[2][0] / latitude[2][1]
        latitude_in_deg = lat_degree + (lat_minute / 60.0) + (lat_second / 3600.0)

        lon_degree = longitude[0][0] / longitude[0][1]
        lon_minute = longitude[1][0] / longitude[1][1]
        lon_second = longitude[2][0] / longitude[2][1]
        longitude_in_deg = lon_degree + (lon_minute / 60.0) + (lon_second / 3600.0)

        metadata["latitude"] = latitude_in_deg
        metadata["longitude"] = longitude_in_deg

    return metadata

# ì‚¬ì§„ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
def extract_photo_metadata(image):
    """ì‚¬ì§„ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    # 1. EXIF ë°ì´í„° ì½ê¸°
    exif_info = read_exif_data(image)

    # 2. ì£¼ìš” ì •ë³´ ì¶”ì¶œ (ë‚ ì§œ, ì¹´ë©”ë¼, GPS)
    metadata = extract_key_info(exif_info)

    return metadata

# ê°ì²´ íƒì§€ ì‹¤í–‰ í•¨ìˆ˜
def run_object_detection_model(image):
    # ì´ë¯¸ì§€ ì¤€ë¹„í•˜ê¸°
    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)

    # ì¶œë ¥ ë ˆì´ì–´
    output_layers = net.getUnconnectedOutLayersNames()
    layer_outputs = net.forward(output_layers)

    # ê°ì²´ íƒì§€ ê²°ê³¼
    detected_objects = []
    height, width, channels = image.shape
    for output in layer_outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:    # ì‹ ë¢°ë„ê°€ 50%ì¼ ë•Œë§Œ íƒì§€í•˜ê¸°
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)

                # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                x = center_x - w // 2
                y = center_y - h // 2

                detected_objects.append({
                    "class": classes[class_id],
                    "confidence": confidence,
                    "box": [x, y, w, h]
                })

    return detected_objects

# ì‚¬ì§„ ì•ˆì˜ ê°ì²´ íƒì§€í•˜ê¸°
def detect_photo_objects(image):
    """ì‚¬ì§„ ë‚´ì˜ ê°ì²´ íƒì§€"""
    detected_objects = run_object_detection_model(image)

    # ë””ë²„ê¹…
    for obj in detected_objects:
        print(f"Detected {obj['class']} with confidence {obj['confidence']:.2f}")

    return detected_objects

# ë‚ ì§œ í‚¤ì›Œë“œ ì¶”ê°€í•˜ê¸°
def create_date_keywords(taken_data):
    # ì´¬ì˜ ì¼ì íŒŒì‹±
    try:
        date_obj = datetime.strptime(taken_data, "%Y:%m:%d %H:%M:%S")

        # ì—°ë„, ì›”, ì¼, ì‹œê°„ëŒ€ ì¶”ì¶œ
        year = str(date_obj.year)
        month = str(date_obj.month).zfill(2)  # 09ì²˜ëŸ¼ ë‘ ìë¦¿ìˆ˜ë¡œ í‘œì‹œ
        day = str(date_obj.day).zfill(2)
        hour = date_obj.hour

        # ì‹œê°„ëŒ€ ì¶”ì¶œ(ì˜¤ì „/ì˜¤í›„)
        time_of_day = "AM" if hour < 12 else "PM"

        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        date_keywords = [year, month, day, time_of_day]
        return date_keywords
    
    except Exception as e:
        print(f"Error parsing date: {e}")
        return []
    
# ìœ„ì¹˜ ì •ë³´ ì£¼ì†Œ ë°˜í™˜
def reverse_geocoding(location):
    """
    location: (latitude, longtitude) íŠœí”Œ í˜•íƒœì˜ ì¢Œí‘œ
    ì˜ˆ: (37.7749, -122.4194)
    """

    geolocator = Nominatim(user_agent="geoapiExercises")

    # Reverse Geocoding (ìœ„ë„, ê²½ë„ë¥¼ ì£¼ì†Œë¡œ ë³€í™˜)
    location_info = geolocator.reverse(location, language="ko")

    if location_info:
        return location_info.address
    else:
        return None

# ì‚¬ì§„ í‚¤ì›Œë“œ ìƒì„±í•˜ê¸°
def generate_photo_keywords(metadata, objects):
    """ì‚¬ì§„ ë©”íƒ€ë°ì´í„°ì™€ ê°ì²´ë¡œ í‚¤ì›Œë“œ ìƒì„±"""
    keywords = []

    # ë‚ ì§œ í‚¤ì›Œë“œ ì¶”ê°€
    if metadata.get("taken_date"):
        keywords.extend(create_date_keywords(metadata["taken_data"]))

    # ì¹´ë©”ë¼ ì •ë³´ ì¶”ê°€
    if metadata.get("camera_info"):
        keywords.append(metadata["camera_info"])

    # íƒì§€ëœ ê°ì²´ ì¶”ê°€
    keywords.extend(objects)

    # ìœ„ì¹˜ ì •ë³´ë¥¼ ì£¼ì†Œë¡œ ë³€í™˜
    if metadata.get("location"):
        address = reverse_geocoding(metadata["location"])
        if address:
            keywords.append(address)

    return keywords

# ì¼ë°˜ ì‚¬ì§„ì¸ì§€ í™•ì¸
def is_photo(doc_type, content):
    """
    ë¬¸ì„œ ì‚¬ì§„ê³¼ ì¼ë°˜ ì‚¬ì§„ì„ êµ¬ë¶„í•˜ëŠ” í•¨ìˆ˜
    image: ì´ë¯¸ì§€ íŒŒì¼
    doc_type: ë¬¸ì„œ íƒ€ì…
    content: ë¬¸ì„œ ë‚´ìš©
    """
    if doc_type == "image" or len(content.strip()) == 0:
        return True
    else:
        return False

# êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒì„±í•˜ê¸°
def format_photo_data(metadata, objects):
    """
    êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒì„±: ë©”íƒ€ë°ì´í„°ì™€ ê°ì²´ íƒì§€ ê²°ê³¼ë¥¼ í•©ì¹œë‹¤.
    metadata: ì‚¬ì§„ì˜ ë©”íƒ€ë°ì´í„°(EXIF ë°ì´í„° ë“±)
    objects: ê°ì²´ íƒì§€ ê²°ê³¼ (ì˜ˆ: ì‚¬ëŒ, ìë™ì°¨ ë“±)
    """
    # ê¸°ë³¸ ë©”íƒ€ ë°ì´í„°
    photo_data = {
        "date_time": metadata.get("taken_data", ""),
        "make": metadata.get("make", ""),
        "model": metadata.get("model", ""),
        "latitude": metadata.get("latitude", None),
        "longtitude": metadata.get("longitude", None),
        "objects_detected": objects  # íƒì§€ëœ ê°ì²´ë“¤
    }

    return photo_data

# ì‚¬ì§„ ìš”ì•½ ìƒì„±
def create_photo_summary(objects, metadata):
    """
    ì‚¬ì§„ ìš”ì•½ ìƒì„±: ê°ì²´ íƒì§€ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ìš”ì•½ì„ ìƒì„±
    objects: íƒì§€ëœ ê°ì²´ë“¤ (ì˜ˆ: ì‚¬ëŒ, ìë™ì°¨ ë“±)
    metadata: ì‚¬ì§„ì˜ ë©”íƒ€ë°ì´í„°
    """
    # ë‚ ì§œ, ì¹´ë©”ë¼ ì •ë³´ ì¶”ì¶œ
    date_time = metadata.get("taken_data", "Unknown date")
    camera_make = metadata.get("make", "Unknown camera")
    camera_model = metadata.get("model", "Unknown model")
    latitude = metadata.get("latitude", "Unknown latitude")
    longitude = metadata.get("longitude", "Unknown longitude")

    # ê°ì²´ ì •ë³´ ìƒì„±
    object_info = []
    for obj in objects:
        object_info.append(f"{obj['class']} (confidence: {obj['confidence']:.2f})")
    
    # ìš”ì•½ ë¬¸ì¥ ìƒì„±
    summary = (
        f"This photo was taken on {date_time} by a {camera_make} {camera_model}. "
        f"The photo includes: {', '.join(object_info)}. "
        f"The photo was taken at latitude {latitude} and longitude {longitude}."
    )
    
    return summary

#------------------------------------------------------------
# ë¬¸ì„œ ì²˜ë¦¬
def process_document(uploaded_file, models, blur_size, block_size, C_value, dilation_iter, to_grayscale):
    (dit_processor, dit_model, ocr, donut_processor, donut_model, 
     layout_processor, layout_model, sum_tokenizer, sum_model,
     embedding_model) = models
    
    image = Image.open(uploaded_file).convert("RGB")
    
    # 1. ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜
    doc_type = classify_document(image, dit_processor, dit_model)
    
    print('\n==========')
    print(f'\n[doc_type]\n{doc_type}')
    
    content, boxes = extract_text_with_layout(image, ocr, blur_size, block_size, C_value, dilation_iter, to_grayscale)
    layoutlm_data = extract_structured_with_layoutlm(image, content, boxes, layout_processor, layout_model, doc_type)
    
    print('\n==========')
    print(f'\n[layoutlm_data]\n{layoutlm_data}')
    
    # ì´ê±° ì–´ë””ì— ë“¤ì–´ê°€ì•¼ í•˜ëŠ”ì§€ ìƒê°í•˜ê¸°
    # ì‚¬ì§„ ì—¬ë¶€ íŒë³„í•˜ê¸°
    if is_photo(doc_type, content):
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = extract_photo_metadata(image)
        
        # ê°ì²´ íƒì§€
        objects = detect_photo_objects(image)

        # í‚¤ì›Œë“œ ìƒì„±
        keywords = generate_photo_keywords(metadata, objects)

        # êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ì €ì¥
        structured_data = format_photo_data(metadata, objects)

        # ìš”ì•½ ìƒì„±
        summary = create_photo_summary(objects, metadata)

        # ì„ë² ë”© ìƒì„±
        embedding = create_embedding(summary, embedding_model)

        return doc_type, content, summary, keywords, structured_data, img_data, embedding

    # 2. ë¬¸ì„œ ìœ í˜•ë³„ ì²˜ë¦¬
    if doc_type == "ì˜ìˆ˜ì¦":
        print('ì˜ìˆ˜ì¦')
        # Donutìœ¼ë¡œ ì˜ìˆ˜ì¦ ì •ë³´ ì¶”ì¶œ
        receipt_info = extract_receipt_info(image, donut_processor, donut_model)
        print('\n==========')
        print(f'\n[receipt_info]\n{receipt_info}')
        
        # êµ¬ì¡°í™”ëœ ì •ë³´ ë³‘í•©
        structured_data = extract_structured_info(content, doc_type)
        if receipt_info:
            # Donut ê²°ê³¼ê°€ ìˆìœ¼ë©´ LayoutMLê³¼ ë³‘í•©    
            for key, value in receipt_info.items():
                if key not in layoutlm_data or not layoutlm_data[key]:
                    layoutlm_data[key] = value
        else:
            structured_data = layoutlm_data
        structured_data.update(receipt_info)
    else:
        # ì¼ë°˜ OCR
        structured_data = layoutlm_data

    print('\n==========')
    print(f'\n[content]\n{content}')
    print('\n==========')
    print(f'\n[structured_data]\n{structured_data}')
    
    
        
        
    # 3. ìš”ì•½ ìƒì„±
    summary = summarize_text(content, sum_tokenizer, sum_model)

    print('\n==========')
    print(f'\n[summary]\n{summary}')
    
    # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(content, structured_data)
    print('\n==========')
    print(f'\n[keywords]\n{keywords}')
    
    # 5. ì„ë² ë”© ìƒì„±
    embedding = create_embedding(content + " " + summary, embedding_model)
    
    # ì´ë¯¸ì§€ ì €ì¥
    img_byte_arr = io.BytesIO()
    image.save(img_byte_arr, format='PNG')
    img_data = img_byte_arr.getvalue()

    return doc_type, content, summary, keywords, structured_data, img_data, embedding

# ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
def search_by_similarity(query, embedding_model, session):
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = create_embedding(query, embedding_model)
    
    # ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
    all_docs = session.exec(select(Document)).all()
    
    similarities = []
    for doc in all_docs:
        if doc.embedding:
            doc_embedding = json.loads(doc.embedding)
            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]
            similarities.append((doc, similarity))
    
    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    similarities.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, sim in similarities[:10] if sim > 0.5]

# ê²°ê³¼ ëª©ë¡ ì¶œë ¥
def print_result_list(results):
    for doc in results:
        with st.expander(f"{doc.filename} - {doc.upload_date.strftime('%Y-%m-%d %H:%M')}"):
            col1, col2 = st.columns(2)
            with col1:
                img = Image.open(io.BytesIO(doc.image_data))
                st.image(img, use_container_width=True)
            with col2:
                st.write(f"**ë¬¸ì„œ ìœ í˜•:** {doc.doc_type}")
                st.write(f"**ìš”ì•½:** {doc.summary}")
                st.write(f"**í‚¤ì›Œë“œ:** {doc.keywords}")
                
                if doc.structured_data:
                    structured = json.loads(doc.structured_data)
                    if structured:
                        st.write("**ì¶”ì¶œëœ ì •ë³´:**")
                        for k, v in structured.items():
                            st.write(f"- {k}: {v}")
                
                # ë‹¤ìš´ë¡œë“œ
                b64 = base64.b64encode(doc.image_data).decode()
                href = f'<a href="data:image/png;base64,{b64}" download="{doc.filename}">ë‹¤ìš´ë¡œë“œ</a>'
                st.markdown(href, unsafe_allow_html=True)

#-------------------------------------------------------------------------------------------------
# Streamlit UI
st.title("AI ì•„ì¹´ì´ë¸Œ ì‹œìŠ¤í…œ")

# ëª¨ë¸ ë¡œë“œ
with st.spinner("AI ëª¨ë¸ ë¡œë”© ì¤‘..."):
    models = load_models()
    net, classes = load_yolo_model()

# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["ë¬¸ì„œ ì—…ë¡œë“œ", "ë¬¸ì„œ ê²€ìƒ‰", "ë¬¸ì„œ ëª©ë¡"])

if 'processed_file' not in st.session_state:
    st.session_state.processed_file = None
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'doc_results' not in st.session_state:
    st.session_state.doc_results = None

# ë¬¸ì„œ ì—…ë¡œë“œ íƒ­
with tab1:
    uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['png', 'jpg', 'jpeg'])

    # ìƒˆ íŒŒì¼ì´ë©´ ì„¸ì…˜ ì´ˆê¸°í™” í•˜ê¸° + ì´ì „ ê²°ê³¼ ë°±ì—…í•˜ê¸°
    if uploaded_file is not None and uploaded_file != st.session_state.processed_file:
        # ì´ì „ ê²°ê³¼ ë°±ì—…
        if st.session_state.doc_results:
            st.session_state.prev_doc_results = st.session_state.doc_results.copy()

        st.session_state.processed_file = uploaded_file
        st.session_state.processing_complete = False
        st.session_state.ocr_ready = False    # OCRì€ ë²„íŠ¼ ëˆ„ë¥¼ ë•Œë§Œ ì§„í–‰í•œë‹¤.

    # íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆë‹¤ë©´
    if uploaded_file:
        st.subheader("ğŸ”§ ì „ì²˜ë¦¬ ì„¤ì •")

        grayscale_option = st.checkbox("í‘ë°±(Grayscale) ì²˜ë¦¬", value=True)
        blur_size = st.slider("ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì»¤ë„ í¬ê¸°", 1, 11, 5, step=2)
        block_size = st.slider("Adaptive Threshold blockSize", 3, 25, 11, step=2)
        C_value = st.slider("Threshold ìƒìˆ˜ C", 0, 10, 2)
        dilation_iter = st.slider("Dilation ë°˜ë³µ íšŸìˆ˜", 0, 3, 1)

        # ì „ì²˜ë¦¬ ë¯¸ë¦¬ë³´ê¸°
        pil_image = Image.open(uploaded_file).convert("RGB")
        final_image = preprocess_image_for_ocr(pil_image, blur_size, block_size, C_value, dilation_iter, grayscale_option)

        if grayscale_option:
            channel_name = "GRAY"
        else:
            channel_name = "COLOR"
        
        st.image(final_image, caption="ì „ì²˜ë¦¬ ê²°ê³¼", channels=channel_name)
        
        if st.button("ğŸ”  OCR ì‹œì‘"):
            with st.spinner("OCR ë¶„ì„ ì¤‘..."):
                doc_type, content, summary, keywords, structured_data, img_data, embedding = process_document(
                    uploaded_file, models, blur_size, block_size, C_value, dilation_iter, grayscale_option
                )
                st.session_state.processing_complete = True
                st.session_state.ocr_ready = True
                # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
                st.session_state.doc_results = {
                    'doc_type': doc_type,
                    'content': content,
                    'summary': summary,
                    'keywords': keywords,
                    'structured_data': structured_data,
                    'img_data': img_data,
                    'embedding': embedding
                }
        
    prev_results = st.session_state.get("prev_doc_results")

    # ì²˜ë¦¬ ì™„ë£Œëœ ê²°ê³¼ í‘œì‹œ
    if uploaded_file is not None and st.session_state.doc_results is not None:
        results = st.session_state.doc_results
        

        st.subheader("ğŸ“„ í˜„ì¬ ë¶„ì„ëœ ë¬¸ì„œ ê²°ê³¼")
        
        col1, col2 = st.columns(2)
        with col1:
            st.image(uploaded_file, caption="ì—…ë¡œë“œëœ ë¬¸ì„œ", use_container_width=True)
        
        with col2:
            st.write(f"**ë¬¸ì„œ ìœ í˜•:** {results['doc_type']}")
            st.write(f"**ìš”ì•½:** {results['summary']}")
            st.write(f"**í‚¤ì›Œë“œ:** {results['keywords']}")
            
            if results['structured_data']:
                st.write("**ì¶”ì¶œëœ ì •ë³´:**")
                for key, value in results['structured_data'].items():
                    st.write(f"- {key}: {value}")
            
            if st.button("ì €ì¥"):
                with Session(engine) as session:
                    doc = Document(
                        filename=uploaded_file.name,
                        doc_type=results['doc_type'],
                        content=results['content'],
                        summary=results['summary'],
                        keywords=results['keywords'],
                        structured_data=json.dumps(results['structured_data'], ensure_ascii=False),
                        image_data=results['img_data'],
                        embedding=json.dumps(results['embedding'])
                    )
                    session.add(doc)
                    session.commit()
                st.success("ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.session_state.processed_file = None
                st.session_state.processing_complete = False
                st.session_state.doc_results = None

    # ì´ì „ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë¹„êµí•˜ê¸°
    if prev_results:
        st.markdown("---")
        st.subheader("ğŸ” ì´ì „ ë¶„ì„ ê²°ê³¼ì™€ ë¹„êµ")

        col1, col2 = st.columns(2)

        with col1:
            st.write("ğŸ“„ **ì´ì „ ìš”ì•½**")
            st.write(prev_results["summary"])
            st.write("ğŸ”‘ **ì´ì „ í‚¤ì›Œë“œ**")
            st.write(prev_results["keywords"])

            if prev_results.get("structured_data"):
                st.write("ğŸ“‹ **ì´ì „ ì¶”ì¶œ ì •ë³´**")
                for key, value in prev_results["structured_data"].items():
                    st.write(f"- {key}: {value}")

        with col2:
            st.write("ğŸ“„ **í˜„ì¬ ìš”ì•½**")
            st.write(results["summary"])
            st.write("ğŸ”‘ **í˜„ì¬ í‚¤ì›Œë“œ**")
            st.write(results["keywords"])

            if results.get("structured_data"):
                st.write("ğŸ“‹ **í˜„ì¬ ì¶”ì¶œ ì •ë³´**")
                for key, value in results["structured_data"].items():
                    st.write(f"- {key}: {value}")

# ë¬¸ì„œ ê²€ìƒ‰ íƒ­
with tab2:
    search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì»¤í”¼ ì˜ìˆ˜ì¦)")
    search_method = st.radio("ê²€ìƒ‰ ë°©ë²•", ["ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰", "í‚¤ì›Œë“œ ê²€ìƒ‰"])
    
    if st.button("ê²€ìƒ‰", key='search_button'):
        with Session(engine) as session:
            if search_method == "ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰":
                results = search_by_similarity(
                    search_query, 
                    models[9],  # embedding_model
                    session
                )
            else:
                # í‚¤ì›Œë“œ ê²€ìƒ‰
                statement = select(Document).where(
                    Document.keywords.contains(search_query) | 
                    Document.summary.contains(search_query) |
                    Document.doc_type.contains(search_query)
                )
                results = session.exec(statement).all()
            
            if results:
                print_result_list(results)
            else:
                st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ë¬¸ì„œ ëª©ë¡ íƒ­
with tab3:
    with Session(engine) as session:
        statement = select(Document)
        results = session.exec(statement).all()
        if results:
            print_result_list(results)
    